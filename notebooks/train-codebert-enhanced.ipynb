{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding\n\nIn this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n\nUnlike the previous version that focused only on identifying Package.swift files, this enhanced version trains the model on the entire dataset by classifying Swift files into meaningful categories based on their purpose in a codebase.\n\n## Overview\n\nThe process of fine-tuning CodeBERT involves:\n\n1. **\ud83d\udd27 Setup**: Install necessary libraries and prepare our environment\n2. **\ud83d\udce5 Data Loading**: Load the Swift code dataset from Hugging Face\n3. **\ud83e\uddf9 Enhanced Preprocessing**: Prepare the data for training by categorizing files and tokenizing the code samples\n4. **\ud83e\udde0 Model Training**: Fine-tune CodeBERT on our prepared data\n5. **\ud83d\udcca Evaluation**: Assess how well our model performs\n6. **\ud83d\udce4 Export & Upload**: Save the model and upload it to Dropbox\n\nLet's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall TensorFlow and install TensorFlow-cpu (better for Kaggle environment)\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow-cpu\n",
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Add memory management function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "print(\"Using default configuration values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Make sure dataset ID is defined (in case previous cell didn't execute)\n",
    "if 'DATASET_ID' not in globals():\n",
    "    print(\"Warning: DATASET_ID not found. Using default value.\")\n",
    "    DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"  # Default value as fallback\n",
    "    MAX_LENGTH = 384\n",
    "    MODEL_ID = \"microsoft/codebert-base\"\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    print(\"Using default configuration values.\")\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "    else:\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "    \n",
    "    print(\"Example features:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset example: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    # Use MODEL_NAME instead of MODEL_ID to match the variable defined earlier\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Enhanced Data Preparation\n",
    "\n",
    "Instead of focusing only on Package.swift files, we'll create a more meaningful multi-class classification task that categorizes Swift files based on their purpose in a codebase. This approach utilizes the entire dataset and provides more valuable insights into code understanding.\n",
    "\n",
    "We'll categorize files into the following classes:\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-data-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "def analyze_content_for_category(content):\n",
    "    \"\"\"\n",
    "    Analyze file content to help determine its category when path-based classification is ambiguous.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The file content\n",
    "        \n",
    "    Returns:\n",
    "        int: The suggested category based on content analysis\n",
    "    \"\"\"\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Check for model patterns\n",
    "    if (re.search(r'struct\\s+\\w+', content) or \n",
    "        re.search(r'class\\s+\\w+\\s*:\\s*\\w*codable', content_lower) or\n",
    "        'encodable' in content_lower or 'decodable' in content_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Check for view patterns\n",
    "    elif ('uiview' in content_lower or \n",
    "          'uitableview' in content_lower or \n",
    "          'uicollectionview' in content_lower or\n",
    "          'swiftui' in content_lower or\n",
    "          'view {' in content_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Check for controller patterns\n",
    "    elif ('viewcontroller' in content_lower or \n",
    "          'uiviewcontroller' in content_lower or\n",
    "          'navigationcontroller' in content_lower or\n",
    "          'viewdidload' in content_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Check for utility patterns\n",
    "    elif ('extension' in content_lower or \n",
    "          'func ' in content and not 'class' in content_lower[:100] or\n",
    "          'protocol' in content_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Check for test patterns\n",
    "    elif ('xctest' in content_lower or \n",
    "          'testcase' in content_lower or\n",
    "          'func test' in content_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Check for configuration patterns\n",
    "    elif ('package(' in content_lower or \n",
    "          'dependencies' in content_lower and 'package' in content_lower or\n",
    "          'products' in content_lower and 'targets' in content_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to -1 (undetermined)\n",
    "    return -1\n",
    "\n",
    "def enhanced_add_labels(example):\n",
    "    \"\"\"\n",
    "    Enhanced labeling function that categorizes Swift files based on their purpose.\n",
    "    \n",
    "    Categories:\n",
    "    0: Models - Data structures and model definitions\n",
    "    1: Views - UI related files\n",
    "    2: Controllers - Application logic\n",
    "    3: Utilities - Helper functions and extensions\n",
    "    4: Tests - Test files\n",
    "    5: Configuration - Package and configuration files\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'path' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        example: The example with added 'label' field\n",
    "    \"\"\"\n",
    "    # First try to determine category from path\n",
    "    path_category = extract_file_type(example['path'])\n",
    "    \n",
    "    # If the path-based category is ambiguous (category 3 - Utilities is our default),\n",
    "    # try to analyze the content for a more specific category\n",
    "    if path_category == 3:\n",
    "        content_category = analyze_content_for_category(example['content'])\n",
    "        # Only use content category if it's determined (-1 means undetermined)\n",
    "        if content_category != -1:\n",
    "            example['label'] = content_category\n",
    "        else:\n",
    "            example['label'] = path_category\n",
    "    else:\n",
    "        example['label'] = path_category\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-enhanced-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Apply the enhanced labeling function\n",
    "    labeled_data = data['train'].map(enhanced_add_labels)\n",
    "    \n",
    "    # Check the distribution of labels\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        category_names = {\n",
    "            0: \"Models\",\n",
    "            1: \"Views\",\n",
    "            2: \"Controllers\",\n",
    "            3: \"Utilities\",\n",
    "            4: \"Tests\",\n",
    "            5: \"Configuration\"\n",
    "        }\n",
    "        category_name = category_names.get(label, f\"Category {label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "    max_label_count = max(label_counter.values())\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "Now let's split our data into training and validation sets with stratification to maintain label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(set(labeled_data[\"label\"]))\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Create a new dataset with ClassLabel feature\n",
    "    labeled_data = labeled_data.cast_column(\"label\", ClassLabel(num_classes=num_labels, names=[str(i) for i in unique_labels]))\n",
    "    \n",
    "    # First split: Create train and temp sets (temp will be split into val and test)  \n",
    "    train_temp_split = labeled_data.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "    train_data = train_temp_split['train']\n",
    "    \n",
    "    # Second split: Split temp into validation and test sets\n",
    "    val_test_split = train_temp_split['test'].train_test_split(test_size=0.5, seed=42, stratify_by_column='label')\n",
    "    val_data = val_test_split['train']\n",
    "    test_data = val_test_split['test']\n",
    "    \n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "    test_label_counter = collections.Counter(test_data['label'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "    \n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "    \n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test label distribution: {dict(test_label_counter)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-section",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now let's tokenize our data for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples['content'],  # Use 'content' column instead of 'func'\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the datasets\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_data.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "print(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "print(f\"Tokenized {len(tokenized_test_data)} test examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Now let's set up the CodeBERT model for our multi-class classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model with the correct number of labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded with {num_labels} output classes\")\n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class-weights-section",
   "metadata": {},
   "source": [
    "## Class Weights Calculation\n",
    "\n",
    "Since we detected label imbalance, let's calculate class weights to help the model learn better from imbalanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-class-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Convert PyTorch tensor to numpy array if needed\n",
    "if hasattr(tokenized_train_data['label'], 'numpy'):\n",
    "    labels = tokenized_train_data['label'].numpy()\n",
    "else:\n",
    "    # If it's already a list or another type, convert to numpy array\n",
    "    labels = np.array(tokenized_train_data['label'])\n",
    "\n",
    "# Compute balanced class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    category_names = {\n",
    "        0: \"Models\",\n",
    "        1: \"Views\",\n",
    "        2: \"Controllers\",\n",
    "        3: \"Utilities\",\n",
    "        4: \"Tests\",\n",
    "        5: \"Configuration\"\n",
    "    }\n",
    "    category_name = category_names.get(i, f\"Category {i}\")\n",
    "    print(f\"  Class {i} ({category_name}): {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Let's set up the training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom loss function with class weights\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Use class weights in the loss calculation\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Create trainer with weighted loss\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"./final_model\")\n",
    "    print(\"Model saved to ./final_model\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test_data)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"Test results:\")\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(tokenized_test_data)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\"Models\", \"Views\", \"Controllers\", \"Utilities\", \"Tests\", \"Configuration\"]\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-examples",
   "metadata": {},
   "source": [
    "## Prediction Examples\n",
    "\n",
    "Let's look at some examples of model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get a few examples from the test set\n",
    "    num_examples = 5\n",
    "    examples = test_data.select(range(num_examples))\n",
    "    \n",
    "    # Tokenize examples\n",
    "    inputs = tokenizer(examples['content'], padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_classes = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\"Models\", \"Views\", \"Controllers\", \"Utilities\", \"Tests\", \"Configuration\"]\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"Example predictions:\")\n",
    "    for i in range(num_examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"File path: {examples['path'][i]}\")\n",
    "        print(f\"Content preview: {examples['content'][i][:100]}...\")\n",
    "        print(f\"Predicted class: {predicted_classes[i]} ({class_names[predicted_classes[i]]})\")\n",
    "        print(f\"True class: {examples['label'][i]} ({class_names[examples['label'][i]]})\")\n",
    "        print(f\"Confidence: {predictions[i][predicted_classes[i]].item():.4f}\")\n",
    "        \n",
    "        # Print top 3 predictions\n",
    "        top_3 = torch.topk(predictions[i], 3)\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for j in range(3):\n",
    "            idx = top_3.indices[j].item()\n",
    "            prob = top_3.values[j].item()\n",
    "            print(f\"  {class_names[idx]}: {prob:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction examples: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-export",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Let's save and export our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save model and tokenizer\n",
    "    output_dir = \"./swift_codebert_classifier\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save class names and other metadata\n",
    "    metadata = {\n",
    "        \"class_names\": class_names,\n",
    "        \"num_classes\": num_labels,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset\": DATASET_ID,\n",
    "        \"accuracy\": float(test_results[\"eval_accuracy\"]),\n",
    "        \"f1\": float(test_results[\"eval_f1\"]),\n",
    "        \"precision\": float(test_results[\"eval_precision\"]),\n",
    "        \"recall\": float(test_results[\"eval_recall\"])\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Model, tokenizer, and metadata saved to {output_dir}\")\n",
    "    \n",
    "    # Create a zip file for easy download\n",
    "    import shutil\n",
    "    shutil.make_archive(\"swift_codebert_classifier\", \"zip\", \".\", output_dir)\n",
    "    print(\"Model package created: swift_codebert_classifier.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropbox-section",
   "metadata": {},
   "source": [
    "## Uploading to Dropbox\n",
    "\n",
    "Now let's upload our trained model to Dropbox for easy access and distribution with improved error handling and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropbox-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Dropbox credentials\n",
    "# You should set these as environment variables in a production environment\n",
    "APP_KEY = \"2bi422xpd3xd962\"  # Replace with your actual app key\n",
    "APP_SECRET = \"j3yx0b41qdvfu86\"  # Replace with your actual app secret\n",
    "REFRESH_TOKEN = \"RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\"  # Replace with your actual refresh token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "from dropbox.exceptions import ApiError\n",
    "from dropbox.files import WriteMode\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def validate_dropbox_credentials(app_key, app_secret, refresh_token):\n",
    "    \"\"\"Test Dropbox credentials before attempting upload.\"\"\"\n",
    "    try:\n",
    "        print(\"Validating Dropbox credentials...\")\n",
    "        dbx = dropbox.Dropbox(\n",
    "            app_key=app_key,\n",
    "            app_secret=app_secret,\n",
    "            oauth2_refresh_token=refresh_token\n",
    "        )\n",
    "        # Check that the access token is valid\n",
    "        account = dbx.users_get_current_account()\n",
    "        print(f\"✅ Connected to Dropbox account: {account.name.display_name}\")\n",
    "        return True, dbx\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to Dropbox: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Validate Dropbox credentials\n",
    "credentials_valid, dbx = validate_dropbox_credentials(APP_KEY, APP_SECRET, REFRESH_TOKEN)\n",
    "\n",
    "if not credentials_valid:\n",
    "    print(\"Please check your Dropbox credentials and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dropbox(file_path, dropbox_path, max_retries=3):\n",
    "    \"\"\"Upload a file to Dropbox with retry logic.\"\"\"\n",
    "    if not credentials_valid:\n",
    "        print(\"Dropbox credentials are not valid. Cannot upload.\")\n",
    "        return False\n",
    "        \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                # For small files, upload in one go\n",
    "                if file_size <= chunk_size:\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path}...\")\n",
    "                    try:\n",
    "                        dbx.files_upload(f.read(), dropbox_path, mode=WriteMode('overwrite'))\n",
    "                        print(\"Upload complete!\")\n",
    "                        return True\n",
    "                    except ApiError as e:\n",
    "                        print(f\"ERROR: Dropbox API error - {e}\")\n",
    "                        if attempt < max_retries - 1:\n",
    "                            print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "                            continue\n",
    "                        return False\n",
    "                \n",
    "                # For large files, use chunked upload\n",
    "                else:\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path} in chunks...\")\n",
    "                    upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "                    cursor = dropbox.files.UploadSessionCursor(\n",
    "                        session_id=upload_session_start_result.session_id,\n",
    "                        offset=f.tell()\n",
    "                    )\n",
    "                    commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode('overwrite'))\n",
    "                    \n",
    "                    # Upload the file in chunks with progress tracking\n",
    "                    uploaded = f.tell()\n",
    "                    with tqdm(total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True) as pbar:\n",
    "                        pbar.update(uploaded)\n",
    "                        while uploaded < file_size:\n",
    "                            if (file_size - uploaded) <= chunk_size:\n",
    "                                dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "                                uploaded = file_size\n",
    "                                pbar.update(file_size - pbar.n)\n",
    "                            else:\n",
    "                                dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "                                uploaded = f.tell()\n",
    "                                cursor.offset = uploaded\n",
    "                                pbar.update(chunk_size)\n",
    "                    print(\"Chunked upload complete!\")\n",
    "                    return True\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Upload failed - {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Upload failed.\")\n",
    "                return False\n",
    "    return False\n",
    "\n",
    "def create_shared_link(dropbox_path):\n",
    "    \"\"\"Create a shared link for a file in Dropbox.\"\"\"\n",
    "    if not credentials_valid:\n",
    "        print(\"Dropbox credentials are not valid. Cannot create shared link.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
    "        return shared_link.url\n",
    "    except ApiError as e:\n",
    "        # If the file already has a shared link, the API will return an error\n",
    "        if isinstance(e.error, dropbox.sharing.CreateSharedLinkWithSettingsError) and \\\n",
    "           e.error.is_path() and e.error.get_path().is_shared_link_already_exists():\n",
    "            # Get existing shared links\n",
    "            shared_links = dbx.sharing_list_shared_links(path=dropbox_path).links\n",
    "            if shared_links:\n",
    "                return shared_links[0].url\n",
    "        print(f\"ERROR: Could not create shared link - {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model zip to Dropbox\n",
    "if credentials_valid:\n",
    "    zip_path = \"swift_codebert_classifier.zip\"\n",
    "    dropbox_path = f\"/swift_codebert_classifier/{os.path.basename(zip_path)}\"\n",
    "    \n",
    "    if upload_to_dropbox(zip_path, dropbox_path):\n",
    "        print(f\"Successfully uploaded model to Dropbox at {dropbox_path}\")\n",
    "        shared_link = create_shared_link(dropbox_path)\n",
    "        if shared_link:\n",
    "            print(f\"Shared link: {shared_link}\")\n",
    "    else:\n",
    "        print(\"Failed to upload model to Dropbox.\")\n",
    "else:\n",
    "    print(\"Skipping Dropbox upload due to invalid credentials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully enhanced the CodeBERT training process to utilize the entire Swift code dataset instead of focusing only on Package.swift files. Our model now classifies Swift code files into meaningful categories based on their purpose in a codebase:\n",
    "\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files\n",
    "\n",
    "This multi-class classification approach provides more valuable insights for code understanding tasks and makes better use of the available data. The model can be used for various code intelligence tasks such as:\n",
    "\n",
    "- Automatically categorizing new code files\n",
    "- Suggesting file organization in large codebases\n",
    "- Identifying misplaced code (e.g., model logic in controller files)\n",
    "- Assisting in code navigation and understanding\n",
    "\n",
    "The same approach can be extended to other programming languages and tasks, such as code search, code completion, and bug detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}