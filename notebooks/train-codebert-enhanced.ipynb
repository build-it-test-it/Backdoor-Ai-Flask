{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding\n\nIn this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n\nUnlike the previous version that focused only on identifying Package.swift files, this enhanced version trains the model on the entire dataset by classifying Swift files into meaningful categories based on their purpose in a codebase.\n\n## Overview\n\nThe process of fine-tuning CodeBERT involves:\n\n1. **ðŸ”§ Setup**: Install necessary libraries and prepare our environment\n2. **ðŸ“¥ Data Loading**: Load the Swift code dataset from Hugging Face\n3. **ðŸ§¹ Enhanced Preprocessing**: Prepare the data for training by categorizing files and tokenizing the code samples\n4. **ðŸ§  Model Training**: Fine-tune CodeBERT on our prepared data\n5. **ðŸ“Š Evaluation**: Assess how well our model performs\n6. **ðŸ“¤ Export & Upload**: Save the model and upload it to Dropbox\n\nLet's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall TensorFlow and install TensorFlow-cpu (better for Kaggle environment)\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow-cpu\n",
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Add memory management function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "print(\"Using default configuration values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Make sure dataset ID is defined (in case previous cell didn't execute)\n",
    "if 'DATASET_ID' not in globals():\n",
    "    print(\"Warning: DATASET_ID not found. Using default value.\")\n",
    "    DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"  # Default value as fallback\n",
    "    MAX_LENGTH = 384\n",
    "    MODEL_ID = \"microsoft/codebert-base\"\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    print(\"Using default configuration values.\")\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "    else:\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "    \n",
    "    print(\"Example features:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset example: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    # Use MODEL_NAME instead of MODEL_ID to match the variable defined earlier\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Enhanced Data Preparation\n",
    "\n",
    "Instead of focusing only on Package.swift files, we'll create a more meaningful multi-class classification task that categorizes Swift files based on their purpose in a codebase. This approach utilizes the entire dataset and provides more valuable insights into code understanding.\n",
    "\n",
    "We'll categorize files into the following classes:\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-data-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "def analyze_content_for_category(content):\n",
    "    \"\"\"\n",
    "    Analyze file content to help determine its category when path-based classification is ambiguous.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The file content\n",
    "        \n",
    "    Returns:\n",
    "        int: The suggested category based on content analysis\n",
    "    \"\"\"\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Check for model patterns\n",
    "    if (re.search(r'struct\\s+\\w+', content) or \n",
    "        re.search(r'class\\s+\\w+\\s*:\\s*\\w*codable', content_lower) or\n",
    "        'encodable' in content_lower or 'decodable' in content_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Check for view patterns\n",
    "    elif ('uiview' in content_lower or \n",
    "          'uitableview' in content_lower or \n",
    "          'uicollectionview' in content_lower or\n",
    "          'swiftui' in content_lower or\n",
    "          'view {' in content_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Check for controller patterns\n",
    "    elif ('viewcontroller' in content_lower or \n",
    "          'uiviewcontroller' in content_lower or\n",
    "          'navigationcontroller' in content_lower or\n",
    "          'viewdidload' in content_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Check for utility patterns\n",
    "    elif ('extension' in content_lower or \n",
    "          'func ' in content and not 'class' in content_lower[:100] or\n",
    "          'protocol' in content_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Check for test patterns\n",
    "    elif ('xctest' in content_lower or \n",
    "          'testcase' in content_lower or\n",
    "          'func test' in content_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Check for configuration patterns\n",
    "    elif ('package(' in content_lower or \n",
    "          'dependencies' in content_lower and 'package' in content_lower or\n",
    "          'products' in content_lower and 'targets' in content_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to -1 (undetermined)\n",
    "    return -1\n",
    "\n",
    "def enhanced_add_labels(example):\n",
    "    \"\"\"\n",
    "    Enhanced labeling function that categorizes Swift files based on their purpose.\n",
    "    \n",
    "    Categories:\n",
    "    0: Models - Data structures and model definitions\n",
    "    1: Views - UI related files\n",
    "    2: Controllers - Application logic\n",
    "    3: Utilities - Helper functions and extensions\n",
    "    4: Tests - Test files\n",
    "    5: Configuration - Package and configuration files\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'path' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        example: The example with added 'label' field\n",
    "    \"\"\"\n",
    "    # First try to determine category from path\n",
    "    path_category = extract_file_type(example['path'])\n",
    "    \n",
    "    # If the path-based category is ambiguous (category 3 - Utilities is our default),\n",
    "    # try to analyze the content for a more specific category\n",
    "    if path_category == 3:\n",
    "        content_category = analyze_content_for_category(example['content'])\n",
    "        # Only use content category if it's determined (-1 means undetermined)\n",
    "        if content_category != -1:\n",
    "            example['label'] = content_category\n",
    "        else:\n",
    "            example['label'] = path_category\n",
    "    else:\n",
    "        example['label'] = path_category\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-enhanced-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Apply the enhanced labeling function\n",
    "    labeled_data = data['train'].map(enhanced_add_labels)\n",
    "    \n",
    "    # Check the distribution of labels\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        category_names = {\n",
    "            0: \"Models\",\n",
    "            1: \"Views\",\n",
    "            2: \"Controllers\",\n",
    "            3: \"Utilities\",\n",
    "            4: \"Tests\",\n",
    "            5: \"Configuration\"\n",
    "        }\n",
    "        category_name = category_names.get(label, f\"Category {label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "    max_label_count = max(label_counter.values())\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "Now let's split our data into training and validation sets with stratification to maintain label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(set(labeled_data[\"label\"]))\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Create a new dataset with ClassLabel feature\n",
    "    labeled_data = labeled_data.cast_column(\"label\", ClassLabel(num_classes=num_labels, names=[str(i) for i in unique_labels]))\n",
    "    \n",
    "    # First split: Create train and temp sets (temp will be split into val and test)  \n",
    "    train_temp_split = labeled_data.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "    train_data = train_temp_split['train']\n",
    "    \n",
    "    # Second split: Split temp into validation and test sets\n",
    "    val_test_split = train_temp_split['test'].train_test_split(test_size=0.5, seed=42, stratify_by_column='label')\n",
    "    val_data = val_test_split['train']\n",
    "    test_data = val_test_split['test']\n",
    "    \n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "    test_label_counter = collections.Counter(test_data['label'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "    \n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "    \n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test label distribution: {dict(test_label_counter)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-section",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now let's tokenize our data for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples['content'],  # Use 'content' column instead of 'func'\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the datasets\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_data.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "print(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "print(f\"Tokenized {len(tokenized_test_data)} test examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Now let's set up the CodeBERT model for our multi-class classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model with the correct number of labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded with {num_labels} output classes\")\n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Let's set up the training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available\n",
    "    dataloader_num_workers=4,  # Use multiple workers for faster data loading\n",
    "    dataloader_pin_memory=True,  # Pin memory for faster data transfer to GPU\n",
    "    report_to=\"none\"  # Disable reporting to avoid potential issues\n",
    ")\n",
    "\n",
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Set up early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"./final_model\")\n",
    "    print(\"Model saved to ./final_model\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test_data)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"Test results:\")\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(tokenized_test_data)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\"Models\", \"Views\", \"Controllers\", \"Utilities\", \"Tests\", \"Configuration\"]\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-examples",
   "metadata": {},
   "source": [
    "## Prediction Examples\n",
    "\n",
    "Let's look at some examples of model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get a few examples from the test set\n",
    "    num_examples = 5\n",
    "    examples = test_data.select(range(num_examples))\n",
    "    \n",
    "    # Tokenize examples\n",
    "    inputs = tokenizer(examples['content'], padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_classes = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\"Models\", \"Views\", \"Controllers\", \"Utilities\", \"Tests\", \"Configuration\"]\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"Example predictions:\")\n",
    "    for i in range(num_examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"File path: {examples['path'][i]}\")\n",
    "        print(f\"Content preview: {examples['content'][i][:100]}...\")\n",
    "        print(f\"Predicted class: {predicted_classes[i]} ({class_names[predicted_classes[i]]})\")\n",
    "        print(f\"True class: {examples['label'][i]} ({class_names[examples['label'][i]]})\")\n",
    "        print(f\"Confidence: {predictions[i][predicted_classes[i]].item():.4f}\")\n",
    "        \n",
    "        # Print top 3 predictions\n",
    "        top_3 = torch.topk(predictions[i], 3)\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for j in range(3):\n",
    "            idx = top_3.indices[j].item()\n",
    "            prob = top_3.values[j].item()\n",
    "            print(f\"  {class_names[idx]}: {prob:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction examples: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-export",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Let's save and export our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save model and tokenizer\n",
    "    output_dir = \"./swift_codebert_classifier\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save class names and other metadata\n",
    "    metadata = {\n",
    "        \"class_names\": class_names,\n",
    "        \"num_classes\": num_labels,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset\": DATASET_ID,\n",
    "        \"accuracy\": float(test_results[\"eval_accuracy\"]),\n",
    "        \"f1\": float(test_results[\"eval_f1\"]),\n",
    "        \"precision\": float(test_results[\"eval_precision\"]),\n",
    "        \"recall\": float(test_results[\"eval_recall\"])\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Model, tokenizer, and metadata saved to {output_dir}\")\n",
    "    \n",
    "    # Create a zip file for easy download\n",
    "    import shutil\n",
    "    shutil.make_archive(\"swift_codebert_classifier\", \"zip\", \".\", output_dir)\n",
    "    print(\"Model package created: swift_codebert_classifier.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully enhanced the CodeBERT training process to utilize the entire Swift code dataset instead of focusing only on Package.swift files. Our model now classifies Swift code files into meaningful categories based on their purpose in a codebase:\n",
    "\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files\n",
    "\n",
    "This multi-class classification approach provides more valuable insights for code understanding tasks and makes better use of the available data. The model can be used for various code intelligence tasks such as:\n",
    "\n",
    "- Automatically categorizing new code files\n",
    "- Suggesting file organization in large codebases\n",
    "- Identifying misplaced code (e.g., model logic in controller files)\n",
    "- Assisting in code navigation and understanding\n",
    "\n",
    "The same approach can be extended to other programming languages and tasks, such as code search, code completion, and bug detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}