{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding\n",
    "\n",
    "This notebook provides a full training implementation for the CodeBERT model on Swift code classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import collections\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Import our custom trainer module\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# Import directly from the local file instead of as a package\n",
    "from codebert_trainer import get_trainer, monitor_resources, cleanup_memory\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU - Note: Training will be much slower on CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration for full training\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16  # Full batch size for GPU training\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 5  # Full number of epochs\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Training mode configuration\n",
    "DEBUG_MODE = False  # Set to True for debug training, False for full training\n",
    "DEBUG_SAMPLE_SIZE = 10000  # Number of examples to use in debug mode\n",
    "\n",
    "# Adjust batch size for CPU if needed\n",
    "if device.type == 'cpu':\n",
    "    BATCH_SIZE = 4  # Reduced batch size for CPU training\n",
    "    print(\"Reduced batch size for CPU training\")\n",
    "\n",
    "print(f\"Training mode: {'DEBUG' if DEBUG_MODE else 'FULL'}\")\n",
    "if DEBUG_MODE:\n",
    "    print(f\"Debug sample size: {DEBUG_SAMPLE_SIZE} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "    \n",
    "    # If in debug mode, take a small sample of the dataset\n",
    "    if DEBUG_MODE and 'train' in data:\n",
    "        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n",
    "        # Take a stratified sample if possible\n",
    "        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n",
    "        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "# Apply the function to create labels\n",
    "try:\n",
    "    # Create a new column with the extracted labels\n",
    "    labeled_data = data['train'].map(lambda example: {\n",
    "        **example,\n",
    "        'label': extract_file_type(example['path'])\n",
    "    })\n",
    "    \n",
    "    # Count the distribution of labels\n",
    "    label_counts = collections.Counter(labeled_data['label'])\n",
    "    \n",
    "    # Define category names for better readability\n",
    "    category_names = {\n",
    "        0: \"Models\",\n",
    "        1: \"Views\",\n",
    "        2: \"Controllers\",\n",
    "        3: \"Utilities\",\n",
    "        4: \"Tests\",\n",
    "        5: \"Configuration\"\n",
    "    }\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        category_name = category_names.get(label, f\"Unknown-{label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(label_counts.keys())\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    print(f\"\\nTotal unique labels: {num_labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "try:\n",
    "    # Shuffle the data\n",
    "    shuffled_data = labeled_data.shuffle(seed=42)\n",
    "    \n",
    "    # Split into train (80%), validation (10%), and test (10%)\n",
    "    train_size = int(0.8 * len(shuffled_data))\n",
    "    val_size = int(0.1 * len(shuffled_data))\n",
    "    \n",
    "    train_data = shuffled_data.select(range(train_size))\n",
    "    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n",
    "    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the code content with proper truncation.\"\"\"\n",
    "    # Tokenize the code content\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # Apply tokenization to each split\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    tokenized_test_data = test_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    print(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "    print(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "    print(f\"Tokenized {len(tokenized_test_data)} test examples\")\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    tokenized_train_data.set_format(\"torch\")\n",
    "    tokenized_val_data.set_format(\"torch\")\n",
    "    tokenized_test_data.set_format(\"torch\")\n",
    "    \n",
    "    print(\"Data tokenization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "try:\n",
    "    # Load the model with the correct number of labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded with {num_labels} output classes\")\n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Print model size\n",
    "    model_size = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model has {model_size:,} parameters\")\n",
    "    \n",
    "    # Check memory usage\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Current memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalanced data\n",
    "try:\n",
    "    # Extract labels for computing class weights\n",
    "    labels = train_data['label']\n",
    "    \n",
    "    # Compute balanced class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    print(\"Class weights:\")\n",
    "    for i, weight in enumerate(class_weights):\n",
    "        category_name = category_names.get(i, f\"Unknown-{i}\")\n",
    "        print(f\"Class {i} ({category_name}): {weight:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error computing class weights: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments based on training mode\n",
    "if DEBUG_MODE:\n",
    "    # Debug training arguments with more frequent evaluation and logging\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        eval_strategy=\"steps\",  # More frequent evaluation for debugging\n",
    "        eval_steps=100,  # Evaluate every 100 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,  # Save every 100 steps\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,  # Log every 10 steps for more visibility\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        # Debug options\n",
    "        disable_tqdm=False,  # Show progress bars\n",
    "        dataloader_num_workers=0,  # No multiprocessing for debugging\n",
    "        dataloader_pin_memory=False  # Disable pin memory for debugging\n",
    "    )\n",
    "    print(\"Using debug training arguments with more frequent evaluation and logging\")\n",
    "else:\n",
    "    # Full training arguments optimized for performance\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        eval_strategy=\"epoch\",  # Evaluate once per epoch for full training\n",
    "        save_strategy=\"epoch\",  # Save once per epoch\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,  # Less frequent logging for full training\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        # Full training options\n",
    "        dataloader_num_workers=4 if torch.cuda.is_available() else 0,  # Use multiprocessing for full training\n",
    "        dataloader_pin_memory=torch.cuda.is_available()  # Enable pin memory for full training\n",
    "    )\n",
    "    print(\"Using full training arguments optimized for performance\")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Define compute_metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Create data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "# Create the appropriate trainer based on debug mode\n",
    "trainer = get_trainer(\n",
    "    debug_mode=DEBUG_MODE,\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training with resource monitoring\n",
    "try:\n",
    "    print(f\"Starting {'DEBUG' if DEBUG_MODE else 'FULL'} training...\")\n",
    "    \n",
    "    # Monitor resources before training\n",
    "    print(\"Resources before training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Start training with a timeout\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run training\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Monitor resources after training\n",
    "    print(\"Resources after training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = \"./final_model_debug\" if DEBUG_MODE else \"./final_model_full\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    \n",
    "    # Print stack trace for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Monitor resources after error\n",
    "    print(\"Resources after error:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "try:\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test_data)\n",
    "    \n",
    "    print(\"Test results:\")\n",
    "    for metric_name, value in test_results.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}