{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollama-header"
      },
      "source": [
        "# Backdoor AI - Mistral on Google Colab\n",
        "\n",
        "This notebook helps you run **Mistral AI models** on Google Colab to use with your Backdoor AI application. Mistral models are known for their strong reasoning capabilities and good performance in a variety of tasks.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. This notebook will install Ollama on this Colab instance\n",
        "2. You'll download the Mistral model\n",
        "3. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
        "4. You'll get a URL to use in your Backdoor AI settings\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "requirements-section"
      },
      "source": [
        "## 1. Set up environment\n",
        "\n",
        "First, let's install the required packages. We'll need Ollama and Cloudflared for tunneling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-requirements"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install cloudflared for tunneling\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q requests pyngrok httpx ipywidgets\n",
        "\n",
        "# Set up directories\n",
        "!mkdir -p /tmp/ollama/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-ollama"
      },
      "source": [
        "## 2. Start Ollama server\n",
        "\n",
        "Now we'll start the Ollama server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-ollama-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Wait for Ollama to start\n",
        "print(\"Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if Ollama is running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/version\")\n",
        "    if response.status_code == 200:\n",
        "        print(f\"‚úÖ Ollama is running! Version: {response.json().get('version')}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ollama returned unexpected status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to connect to Ollama: {e}\")\n",
        "    print(\"Trying to start again...\")\n",
        "    # Kill the previous process if it exists\n",
        "    if ollama_process:\n",
        "        ollama_process.terminate()\n",
        "        time.sleep(2)\n",
        "    # Try starting again\n",
        "    !ollama serve &\n",
        "    time.sleep(5)\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/version\")\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
        "    except:\n",
        "        print(\"‚ùå Failed to start Ollama after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## 3. Download Mistral Model\n",
        "\n",
        "Now, let's download the Mistral model for balanced performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-mistral"
      },
      "outputs": [],
      "source": [
        "# Download Mistral\n",
        "print(\"üöÄ Downloading Mistral model...\")\n",
        "print(\"This model offers excellent reasoning and tool use capabilities.\")\n",
        "print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "\n",
        "# Run the download command\n",
        "!ollama pull mistral:latest\n",
        "\n",
        "# Verify the model is available\n",
        "print(\"\\nüìã Available models:\")\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-model"
      },
      "source": [
        "## 4. Test the Mistral model\n",
        "\n",
        "Let's make sure the model works by asking it something relevant to Backdoor AI usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-mistral"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Define a relevant prompt for Backdoor AI usage\n",
        "test_prompt = \"\"\"\n",
        "I'm working on a Flask application and need help with structuring my API endpoints properly.\n",
        "Can you explain how to effectively organize API routes for a RESTful service, with a specific \n",
        "example for a user management system with authentication?\n",
        "\"\"\"\n",
        "\n",
        "# Set up the API call\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "payload = {\n",
        "    \"model\": \"mistral:latest\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": test_prompt}\n",
        "    ],\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Make the API call\n",
        "try:\n",
        "    print(\"Testing Mistral with a Flask development question...\\n\")\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        content = result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
        "        print(\"‚úÖ Model response:\\n\")\n",
        "        print(content)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: Server returned status {response.status_code}\")\n",
        "        print(response.text)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-tunnel"
      },
      "source": [
        "## 5. Set up a tunnel to access your Ollama instance\n",
        "\n",
        "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Function to run cloudflared tunnel in a separate thread\n",
        "def run_tunnel():\n",
        "    global tunnel_process, tunnel_url\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    # Extract tunnel URL\n",
        "    tunnel_url = None\n",
        "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
        "    \n",
        "    while True:\n",
        "        line = tunnel_process.stdout.readline()\n",
        "        if not line and tunnel_process.poll() is not None:\n",
        "            break\n",
        "        \n",
        "        print(line.strip())\n",
        "        match = url_pattern.search(line)\n",
        "        if match and not tunnel_url:\n",
        "            tunnel_url = match.group(0)\n",
        "            tunnel_info.value = f\"<div style='padding: 10px; background-color: #e6ffe6; border-radius: 5px;'><b>‚úÖ Your Ollama API is accessible at:</b><br><code>{tunnel_url}</code><br><br>Use this URL in your Backdoor AI settings as the Ollama API Base URL.<br><br><b>Important:</b> In your Backdoor settings, make sure to select:<br>- Provider: Ollama<br>- Model: mistral:latest<br><br>Keep this notebook running while you're using Ollama with your app!</div>\"\n",
        "\n",
        "# Initialize global variables\n",
        "tunnel_process = None\n",
        "tunnel_url = None\n",
        "\n",
        "# Create tunnel info widget\n",
        "tunnel_info = widgets.HTML(\"<div style='padding: 10px; background-color: #fff3e6; border-radius: 5px;'>‚è≥ Creating secure tunnel to Ollama... (this may take a moment)</div>\")\n",
        "display(tunnel_info)\n",
        "\n",
        "# Start tunnel\n",
        "thread = threading.Thread(target=run_tunnel)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Wait for tunnel URL\n",
        "attempts = 0\n",
        "while attempts < 30 and not tunnel_url:\n",
        "    time.sleep(1)\n",
        "    attempts += 1\n",
        "    \n",
        "if not tunnel_url:\n",
        "    tunnel_info.value = \"<div style='padding: 10px; background-color: #ffe6e6; border-radius: 5px;'>‚ùå Failed to create tunnel. Check the output below for details.</div>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "backdoor-instructions"
      },
      "source": [
        "## 6. Connect Backdoor AI to your Mistral instance\n",
        "\n",
        "Once you have your tunnel URL, follow these steps to connect Backdoor AI to your Mistral instance:\n",
        "\n",
        "1. Go to your Backdoor AI settings page\n",
        "2. Select the \"Ollama\" tab\n",
        "3. Select \"Use Google Colab\" as your setup method\n",
        "4. In the \"Ollama API URL\" field, enter the tunnel URL from above\n",
        "5. In the \"Ollama Model\" dropdown, select \"mistral:latest\"\n",
        "6. Click \"Save Settings\"\n",
        "\n",
        "### Benefits of Mistral for Backdoor AI\n",
        "\n",
        "Mistral models offer several advantages for Backdoor AI application:\n",
        "- Strong instruction following\n",
        "- Good reasoning on complex problems\n",
        "- Effective tool use capabilities\n",
        "- Efficient context handling\n",
        "- Strong knowledge of software development concepts\n",
        "\n",
        "Mistral is particularly good for:\n",
        "- Debugging and troubleshooting code\n",
        "- Explaining complex programming concepts\n",
        "- Working with Flask and web development tasks\n",
        "- Analyzing and refactoring code\n",
        "\n",
        "**Important Notes:**\n",
        "- Keep this notebook running as long as you're using Ollama with your app\n",
        "- The tunnel URL will change each time you restart this notebook\n",
        "- Google Colab sessions have limited runtime (a few hours for free tier)\n",
        "- Your model downloads will be lost when the Colab session ends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keep-alive"
      },
      "source": [
        "## 7. Keep the Colab session alive\n",
        "\n",
        "Run the cell below to prevent Colab from disconnecting due to inactivity. This will create a small animation that keeps the session active."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keep-alive-code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import IPython.display\n",
        "from IPython.display import HTML, display\n",
        "import threading\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        time.sleep(60)  # Update every minute\n",
        "        IPython.display.clear_output(wait=True)\n",
        "        display(HTML(f'''\n",
        "        <div style=\"padding: 10px; background-color: #f0f9ff; border-radius: 5px; width: 100%;\">\n",
        "            <h3>üì° Ollama Server Status</h3>\n",
        "            <p><b>Tunnel URL:</b> {tunnel_url or \"Not available\"}</p>\n",
        "            <p><b>Model:</b> mistral:latest</p>\n",
        "            <p><b>Session active for:</b> {int(time.time() - start_time)} seconds</p>\n",
        "            <p><b>Status:</b> Running</p>\n",
        "            <div style=\"margin-top:10px; text-align:center;\">\n",
        "                <div class=\"spinner\" style=\"display: inline-block; width: 20px; height: 20px; border: 3px solid rgba(0,0,0,.3); border-radius: 50%; border-top-color: #3498db; animation: spin 1s ease-in-out infinite;\"></div>\n",
        "                <style>\n",
        "                    @keyframes spin { to { transform: rotate(360deg); } }\n",
        "                </style>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''))\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the keep-alive thread\n",
        "keep_alive_thread = threading.Thread(target=keep_alive)\n",
        "keep_alive_thread.daemon = True\n",
        "keep_alive_thread.start()\n",
        "\n",
        "# Initial display\n",
        "display(HTML(f'''\n",
        "<div style=\"padding: 10px; background-color: #f0f9ff; border-radius: 5px; width: 100%;\">\n",
        "    <h3>üì° Ollama Server Status</h3>\n",
        "    <p><b>Tunnel URL:</b> {tunnel_url or \"Not available\"}</p>\n",
        "    <p><b>Model:</b> mistral:latest</p>\n",
        "    <p><b>Session active for:</b> 0 seconds</p>\n",
        "    <p><b>Status:</b> Running</p>\n",
        "    <div style=\"margin-top:10px; text-align:center;\">\n",
        "        <div class=\"spinner\" style=\"display: inline-block; width: 20px; height: 20px; border: 3px solid rgba(0,0,0,.3); border-radius: 50%; border-top-color: #3498db; animation: spin 1s ease-in-out infinite;\"></div>\n",
        "        <style>\n",
        "            @keyframes spin { to { transform: rotate(360deg); } }\n",
        "        </style>\n",
        "    </div>\n",
        "</div>\n",
        "'''))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
