{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Memory-Optimized CodeBERT for Swift Code Understanding\n",
    "\n",
    "This notebook fine-tunes the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint) with optimizations for TPU memory efficiency. This version includes significant memory optimizations to avoid the \"Resource Exhausted\" error during training.\n",
    "\n",
    "## Key Optimizations\n",
    "- üìâ Reduced sequence length from 512 to 384\n",
    "- üìä Reduced batch size and implemented gradient accumulation\n",
    "- üß† Added gradient checkpointing to save memory\n",
    "- üîß Optimized tokenization and data processing\n",
    "- üõ†Ô∏è Enhanced error handling and recovery\n",
    "\n",
    "Let's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests gc psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Add memory management functions\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and configure TPU\n",
    "def detect_and_configure_accelerator():\n",
    "    \"\"\"Detect and configure the available accelerator (CPU, GPU, or TPU).\"\"\"\n",
    "    try:\n",
    "        # Check for TPU\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        print(\"TPU detected! Configuring for TPU training...\")\n",
    "        device = xm.xla_device()\n",
    "        use_tpu = True\n",
    "        use_gpu = False\n",
    "        \n",
    "        # Configure XLA for TPU\n",
    "        import torch_xla.distributed.parallel_loader as pl\n",
    "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "        \n",
    "        print(f\"TPU cores available: {xm.xrt_world_size()}\")\n",
    "        return device, use_tpu, use_gpu\n",
    "        \n",
    "    except ImportError:\n",
    "        # Check for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU detected! Using {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            use_tpu = False\n",
    "            use_gpu = True\n",
    "            print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"No GPU or TPU detected. Using CPU (this will be slow).\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            use_tpu = False\n",
    "            use_gpu = False\n",
    "        \n",
    "        return device, use_tpu, use_gpu\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting accelerator: {e}\")\n",
    "        print(\"Defaulting to CPU.\")\n",
    "        return torch.device(\"cpu\"), False, False\n",
    "\n",
    "# Detect and configure accelerator\n",
    "device, use_tpu, use_gpu = detect_and_configure_accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using with memory-optimized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and dataset IDs\n",
    "MODEL_ID = \"microsoft/codebert-base\"\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Max sequence length - reduced from 512 to save memory\n",
    "MAX_LENGTH = 384  # Reduced from 512 to save memory\n",
    "\n",
    "# Configure batch sizes based on available hardware - optimized for memory efficiency\n",
    "if use_tpu:\n",
    "    # Significantly reduced batch size to prevent TPU memory exhaustion\n",
    "    TRAIN_BATCH_SIZE = 16  # Reduced from 64 to prevent memory issues\n",
    "    EVAL_BATCH_SIZE = 32   # Reduced from 128\n",
    "    # Increased gradient accumulation to maintain effective batch size\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # Accumulate gradients to simulate larger batch\n",
    "elif use_gpu:\n",
    "    TRAIN_BATCH_SIZE = 12   \n",
    "    EVAL_BATCH_SIZE = 24\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "else:\n",
    "    TRAIN_BATCH_SIZE = 6    \n",
    "    EVAL_BATCH_SIZE = 12\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Effective batch size = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "EFFECTIVE_BATCH_SIZE = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Training batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"Evaluation batch size: {EVAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset with memory-efficient handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "try:\n",
    "    # Apply the labeling function\n",
    "    labeled_data = data['train'].map(add_labels)\n",
    "    \n",
    "    # Check the distribution of labels using collections.Counter\n",
    "    import collections\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Split the dataset with stratification to maintain label distribution\n",
    "    from datasets import ClassLabel\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(set(labeled_data[\"label\"]))\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Create a new dataset with ClassLabel feature\n",
    "    labeled_data = labeled_data.cast_column(\"label\", ClassLabel(num_classes=num_labels, names=[str(i) for i in unique_labels]))\n",
    "    \n",
    "    # Split the dataset with stratification\n",
    "    train_test_split = labeled_data.train_test_split(test_size=0.1, seed=42, stratify_by_column='label')\n",
    "    train_data = train_test_split['train']\n",
    "    val_data = train_test_split['test']\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    \n",
    "    # Free up memory\n",
    "    del labeled_data\n",
    "    del data\n",
    "    cleanup_memory()\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer and Tokenization\n",
    "\n",
    "Now, let's load the tokenizer and tokenize our data with memory-efficient settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    print(f\"Tokenizer loaded successfully with {len(tokenizer)} tokens in vocabulary\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize Swift code with memory efficiency.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        padding=False,  # No padding during preprocessing saves memory\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,  # Using reduced max length\n",
    "        return_special_tokens_mask=False,  # Save memory\n",
    "        return_offsets_mapping=False,      # Save memory\n",
    "        return_token_type_ids=True,       # Needed for BERT models\n",
    "        return_attention_mask=True        # Needed for proper masking\n",
    "    )\n",
    "\n",
    "# Tokenize the datasets with smaller batch size\n",
    "try:\n",
    "    # Use fewer parallel processes\n",
    "    import multiprocessing\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_proc = max(1, int(num_cpus * 0.5))  # Use 50% of CPU cores\n",
    "    \n",
    "    # Lower batch size for tokenization\n",
    "    tokenization_batch_size = 32\n",
    "    \n",
    "    print(\"Tokenizing training data...\")\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=tokenization_batch_size,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=[col for col in train_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing training data\"\n",
    "    )\n",
    "    \n",
    "    print(\"Tokenizing validation data...\")\n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=tokenization_batch_size,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=[col for col in val_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing validation data\"\n",
    "    )\n",
    "    \n",
    "    # Print token statistics\n",
    "    train_lengths = [len(x[\"input_ids\"]) for x in tokenized_train_data]\n",
    "    print(f\"Average training sequence length: {sum(train_lengths)/len(train_lengths):.1f} tokens\")\n",
    "    print(f\"Percent of examples truncated: {sum(1 for l in train_lengths if l >= MAX_LENGTH)/len(train_lengths)*100:.2f}%\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del train_data\n",
    "    del val_data\n",
    "    cleanup_memory()\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Now let's load the CodeBERT model with memory-efficient settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load model with memory efficiency\n",
    "    print(\"Loading CodeBERT model with memory optimization...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        num_labels=2,\n",
    "        low_cpu_mem_usage=True  # For memory efficiency\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing (critical for memory savings)\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"Gradient checkpointing enabled for memory efficiency.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not enable gradient checkpointing: {e}\")\n",
    "    \n",
    "    # Move model to device if not TPU\n",
    "    if not use_tpu:\n",
    "        model.to(device)\n",
    "        \n",
    "    print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics with memory-efficient settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute basic evaluation metrics.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Return basic metrics to save memory\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for dynamic padding (saves memory)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments with TPU-optimized memory settings\n",
    "try:\n",
    "    # Set up training arguments for memory efficiency and stability\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/codebert-swift\",\n",
    "        # Basic training parameters\n",
    "        save_steps=200,               \n",
    "        save_total_limit=2,           # Keep fewer checkpoints\n",
    "        learning_rate=3e-5,           \n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Critical for memory efficiency\n",
    "        num_train_epochs=2,           # Reduced from 3 to 2 epochs\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,             \n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        \n",
    "        # TPU-specific configurations\n",
    "        tpu_num_cores=8 if use_tpu else None,  \n",
    "        dataloader_drop_last=True if use_tpu else False,  # Important for TPU\n",
    "        \n",
    "        # Memory optimizations\n",
    "        fp16=use_gpu,                 # Mixed precision on GPU\n",
    "        dataloader_num_workers=2,     # Reduced worker count\n",
    "        dataloader_pin_memory=True,   \n",
    "        max_grad_norm=1.0,           \n",
    "        \n",
    "        # Optimizer settings\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "    )\n",
    "    \n",
    "    print(\"Training arguments configured successfully.\")\n",
    "    print(f\"Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring training arguments: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer with memory-efficient settings\n",
    "try:\n",
    "    # Initialize the trainer without any callbacks\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_val_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,  # Dynamic padding\n",
    "        # No callbacks for simplicity\n",
    "    )\n",
    "    \n",
    "    print(\"Trainer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating trainer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our CodeBERT model with enhanced error handling for memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with checkpoint recovery and enhanced error handling\n",
    "try:\n",
    "    # Clean up memory before training\n",
    "    cleanup_memory()\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(f\"Training completed successfully! Metrics: {train_result.metrics}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    print(\"Saving final model...\")\n",
    "    trainer.save_model()\n",
    "    print(\"Final model saved.\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "    print(\"Training metrics and state saved.\")\n",
    "except RuntimeError as e:\n",
    "    # Handle memory-related errors specially\n",
    "    error_msg = str(e)\n",
    "    print(f\"Runtime error during training: {error_msg}\")\n",
    "    \n",
    "    if \"memory\" in error_msg.lower() or \"cuda out of memory\" in error_msg.lower() or \"resource exhausted\" in error_msg.lower():\n",
    "        print(\"\\nMEMORY ERROR DETECTED! Try further reducing these parameters:\")\n",
    "        print(f\"1. MAX_LENGTH (currently {MAX_LENGTH}). Try 256 or 192.\")\n",
    "        print(f\"2. TRAIN_BATCH_SIZE (currently {TRAIN_BATCH_SIZE}). Try 8 or 4.\")\n",
    "        print(f\"3. Increase GRADIENT_ACCUMULATION_STEPS (currently {GRADIENT_ACCUMULATION_STEPS}). Try 8 or 16.\")\n",
    "    \n",
    "    # Try to save current state\n",
    "    try:\n",
    "        print(\"Attempting to save current model state...\")\n",
    "        trainer.save_model(\"./results/codebert-swift-emergency-save\")\n",
    "        print(\"Emergency model save completed.\")\n",
    "    except Exception as save_err:\n",
    "        print(f\"Could not perform emergency save: {save_err}\")\n",
    "    \n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our model on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with memory efficiency\n",
    "try:\n",
    "    # Clean up memory before evaluation\n",
    "    cleanup_memory()\n",
    "    \n",
    "    print(\"Evaluating model on validation dataset...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    trainer.log_metrics(\"eval\", eval_results)\n",
    "    trainer.save_metrics(\"eval\", eval_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully optimized the CodeBERT fine-tuning process for Swift code classification with significant memory improvements:\n",
    "\n",
    "1. **Memory Optimizations**: Reduced sequence length, batch size, and implemented gradient accumulation\n",
    "2. **Gradient Checkpointing**: Added gradient checkpointing to trade computation for memory\n",
    "3. **Efficient Tokenization**: Optimized the tokenization process to use less memory\n",
    "4. **Improved Error Handling**: Added better error handling and recovery mechanisms\n",
    "5. **TPU-Specific Settings**: Added configurations specific to TPU memory efficiency\n",
    "\n",
    "The model can now be successfully trained on TPU without hitting memory limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
