{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding (Debug Version)\n",
    "\n",
    "This notebook provides a debug training implementation for the CodeBERT model on Swift code classification. It uses a smaller dataset and more frequent logging to help troubleshoot training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import collections\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Import our custom trainer module\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from notebooks.codebert_trainer import get_trainer, monitor_resources, cleanup_memory\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU - Note: Training will be much slower on CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using with debug settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration for debug training\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 4  # Reduced batch size for debugging\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 2  # Reduced number of epochs for debugging\n",
    "WARMUP_STEPS = 100  # Reduced warmup steps for debugging\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Reduced gradient accumulation for debugging\n",
    "\n",
    "# Training mode configuration\n",
    "DEBUG_MODE = True  # Enable debug mode\n",
    "DEBUG_SAMPLE_SIZE = 10000  # Small sample size for debugging\n",
    "\n",
    "print(f\"Training mode: {'DEBUG' if DEBUG_MODE else 'FULL'}\")\n",
    "print(f\"Debug sample size: {DEBUG_SAMPLE_SIZE} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "    \n",
    "    # If in debug mode, take a small sample of the dataset\n",
    "    if DEBUG_MODE and 'train' in data:\n",
    "        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n",
    "        # Take a stratified sample if possible\n",
    "        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n",
    "        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "# Apply the function to create labels\n",
    "try:\n",
    "    # Create a new column with the extracted labels\n",
    "    labeled_data = data['train'].map(lambda example: {\n",
    "        **example,\n",
    "        'label': extract_file_type(example['path'])\n",
    "    })\n",
    "    \n",
    "    # Count the distribution of labels\n",
    "    label_counts = collections.Counter(labeled_data['label'])\n",
    "    \n",
    "    # Define category names for better readability\n",
    "    category_names = {\n",
    "        0: \"Models\",\n",
    "        1: \"Views\",\n",
    "        2: \"Controllers\",\n",
    "        3: \"Utilities\",\n",
    "        4: \"Tests\",\n",
    "        5: \"Configuration\"\n",
    "    }\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        category_name = category_names.get(label, f\"Unknown-{label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(label_counts.keys())\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    print(f\"\\nTotal unique labels: {num_labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "try:\n",
    "    # Shuffle the data\n",
    "    shuffled_data = labeled_data.shuffle(seed=42)\n",
    "    \n",
    "    # Split into train (80%), validation (10%), and test (10%)\n",
    "    train_size = int(0.8 * len(shuffled_data))\n",
    "    val_size = int(0.1 * len(shuffled_data))\n",
    "    \n",
    "    train_data = shuffled_data.select(range(train_size))\n",
    "    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n",
    "    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the code content with proper truncation.\"\"\"\n",
    "    # Tokenize the code content\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # Apply tokenization to each split\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    tokenized_test_data = test_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content']\n",
    "    )\n",
    "    \n",
    "    print(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "    print(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "    print(f\"Tokenized {len(tokenized_test_data)} test examples\")\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    tokenized_train_data.set_format(\"torch\")\n",
    "    tokenized_val_data.set_format(\"torch\")\n",
    "    tokenized_test_data.set_format(\"torch\")\n",
    "    \n",
    "    print(\"Data tokenization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "try:\n",
    "    # Load the model with the correct number of labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded with {num_labels} output classes\")\n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Print model size\n",
    "    model_size = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model has {model_size:,} parameters\")\n",
    "    \n",
    "    # Check memory usage\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Current memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalanced data\n",
    "try:\n",
    "    # Extract labels for computing class weights\n",
    "    labels = train_data['label']\n",
    "    \n",
    "    # Compute balanced class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    print(\"Class weights:\")\n",
    "    for i, weight in enumerate(class_weights):\n",
    "        category_name = category_names.get(i, f\"Unknown-{i}\")\n",
    "        print(f\"Class {i} ({category_name}): {weight:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error computing class weights: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define debug training arguments with more frequent evaluation and logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./debug_results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    eval_strategy=\"steps\",  # More frequent evaluation for debugging\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save every 50 steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./debug_logs\",\n",
    "    logging_steps=10,  # Log every 10 steps for more visibility\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    # Debug options\n",
    "    disable_tqdm=False,  # Show progress bars\n",
    "    dataloader_num_workers=0,  # No multiprocessing for debugging\n",
    "    dataloader_pin_memory=False  # Disable pin memory for debugging\n",
    ")\n",
    "print(\"Using debug training arguments with more frequent evaluation and logging\")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Define compute_metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Create data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "# Create the debug trainer\n",
    "trainer = get_trainer(\n",
    "    debug_mode=DEBUG_MODE,  # This should be True for debug mode\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "print(\"Debug training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training with resource monitoring\n",
    "try:\n",
    "    print(\"Starting DEBUG training...\")\n",
    "    \n",
    "    # Monitor resources before training\n",
    "    print(\"Resources before training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Start training with a timeout\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run training\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Monitor resources after training\n",
    "    print(\"Resources after training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"./debug_model\")\n",
    "    print(\"Model saved to ./debug_model\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    \n",
    "    # Print stack trace for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Monitor resources after error\n",
    "    print(\"Resources after error:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "try:\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test_data)\n",
    "    \n",
    "    print(\"Test results:\")\n",
    "    for metric_name, value in test_results.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}