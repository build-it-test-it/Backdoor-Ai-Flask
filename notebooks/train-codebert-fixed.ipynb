{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# CodeBERT for Swift Code Understanding (Fixed Version)\n\nIn this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n\nWe'll use the Swift code dataset to fine-tune the model for code understanding tasks. After training, we'll upload the model to Dropbox for easy access and distribution.\n\n## Overview\n\nThe process of fine-tuning CodeBERT involves:\n\n1. **\ud83d\udd27 Setup**: Install necessary libraries and prepare our environment\n2. **\ud83d\udce5 Data Loading**: Load the Swift code dataset from Hugging Face\n3. **\ud83e\uddf9 Preprocessing**: Prepare the data for training by tokenizing the code samples\n4. **\ud83e\udde0 Model Training**: Fine-tune CodeBERT on our prepared data\n5. **\ud83d\udcca Evaluation**: Assess how well our model performs\n6. **\ud83d\udce4 Export & Upload**: Save the model and upload it to Dropbox\n\nLet's start by installing the necessary libraries:\n\n**Note:** This is a fixed version of the notebook with improved error handling, TPU detection, and dataset safety checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall TensorFlow and install TensorFlow-cpu (better for Kaggle environment)\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow-cpu\n",
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Add memory management function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved function to detect and configure TPU with better error handling",
    "def detect_and_configure_accelerator():",
    "",
    "",
    "    Copy and paste this function into your notebook to replace the existing",
    "    detect_and_configure_accelerator function.",
    "",
    "",
    "    \"\"\"Detect and configure the available accelerator (CPU, GPU, or TPU) with robust error handling.\"\"\"",
    "    # First try TPU",
    "    try:",
    "        print(\"Checking for TPU availability...\")",
    "        import torch_xla.core.xla_model as xm",
    "        try:",
    "            # Try the new API first (for torch_xla 2.7+)",
    "            try:",
    "                import torch_xla.runtime as xr",
    "                print(\"Using torch_xla.runtime API\")",
    "                have_xr = True",
    "            except ImportError:",
    "                print(\"torch_xla.runtime not available, using legacy API\")",
    "                have_xr = False",
    "                ",
    "            device = xm.xla_device()",
    "            use_tpu = True",
    "            use_gpu = False",
    "            print(\"TPU detected! Configuring for TPU training...\")",
    "            ",
    "            # Try getting cores with exception handling",
    "            try:",
    "                if have_xr:",
    "                    # Use newer API if available",
    "                    cores = xr.world_size()",
    "                    print(f\"TPU cores available (via xr.world_size): {cores}\")",
    "                else:",
    "                    # Fall back to deprecated method with warning capture",
    "                    import warnings",
    "                    with warnings.catch_warnings():",
    "                        warnings.simplefilter(\"ignore\")",
    "                        cores = xm.xrt_world_size()",
    "                        print(f\"TPU cores available (via xm.xrt_world_size): {cores}\")",
    "            except Exception as core_err:",
    "                print(f\"Warning: Could not determine TPU core count: {core_err}\")",
    "                print(\"Proceeding with TPU but unknown core count.\")",
    "                ",
    "            # Configure XLA for TPU - with error handling",
    "            try:",
    "                import torch_xla.distributed.parallel_loader as pl",
    "                import torch_xla.distributed.xla_multiprocessing as xmp",
    "                print(\"Successfully imported TPU distributed libraries\")",
    "            except ImportError as imp_err:",
    "                print(f\"Warning: Could not import some TPU libraries: {imp_err}\")",
    "                print(\"Continuing with basic TPU support\")",
    "            ",
    "            return device, use_tpu, use_gpu",
    "            ",
    "        except Exception as tpu_init_err:",
    "            print(f\"TPU initialization error: {tpu_init_err}\")",
    "            print(\"TPU libraries detected but initialization failed. Falling back to GPU/CPU.\")",
    "            # Fall through to GPU/CPU detection",
    "            ",
    "    except ImportError as ie:",
    "        print(f\"No TPU support detected: {ie}\")",
    "        # Fall through to GPU/CPU detection",
    "        ",
    "    except Exception as e:",
    "        print(f\"Unexpected error in TPU detection: {e}\")",
    "        print(\"Falling back to GPU/CPU.\")",
    "        # Fall through to GPU/CPU detection",
    "    ",
    "    # If TPU not available or failed, try GPU",
    "    try:",
    "        if torch.cuda.is_available():",
    "            print(f\"GPU detected! Using {torch.cuda.get_device_name(0)}\")",
    "            device = torch.device(\"cuda\")",
    "            use_tpu = False",
    "            use_gpu = True",
    "            print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")",
    "        else:",
    "            print(\"No GPU detected. Using CPU (this will be slow).\")",
    "            device = torch.device(\"cpu\")",
    "            use_tpu = False",
    "            use_gpu = False",
    "        return device, use_tpu, use_gpu",
    "        ",
    "    except Exception as e:",
    "        print(f\"Error in GPU/CPU detection: {e}\")",
    "        print(\"Defaulting to CPU.\")",
    "        return torch.device(\"cpu\"), False, False",
    "",
    "",
    "# Detect and configure accelerator",
    "device, use_tpu, use_gpu = detect_and_configure_accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and dataset IDs",
    "# Define maximum sequence length - reduced from 512 to improve memory efficiency",
    "MAX_LENGTH = 384  # Reduced from 512 to save memory",
    "MODEL_ID = \"microsoft/codebert-base\"",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"",
    "",
    "# Configure batch sizes based on available hardware",
    "if use_tpu:",
    "    # Reduced batch size to prevent TPU memory exhaustion",
    "    TRAIN_BATCH_SIZE = 16  # Reduced from 64 to prevent memory issues",
    "    EVAL_BATCH_SIZE = 32   # Reduced from 128",
    "    # Increased gradient accumulation to maintain effective batch size",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # Accumulate gradients to simulate larger batch",
    "elif use_gpu:",
    "    TRAIN_BATCH_SIZE = 16   # Standard batch size for GPU",
    "    EVAL_BATCH_SIZE = 32",
    "    GRADIENT_ACCUMULATION_STEPS = 2",
    "else:",
    "    TRAIN_BATCH_SIZE = 8   # Smaller batch size for CPU",
    "    EVAL_BATCH_SIZE = 16",
    "    GRADIENT_ACCUMULATION_STEPS = 4",
    "",
    "print(f\"Using device: {device}\")",
    "print(f\"Training batch size: {TRAIN_BATCH_SIZE}\")",
    "print(f\"Evaluation batch size: {EVAL_BATCH_SIZE}\")",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")",
    "print(f\"Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Make sure dataset ID is defined (in case previous cell didn't execute)",
    "if 'DATASET_ID' not in globals():",
    "    print(\"Warning: DATASET_ID not found. Using default value.\")",
    "    DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"  # Default value as fallback",
    "    MAX_LENGTH = 384",
    "    MODEL_ID = \"microsoft/codebert-base\"",
    "    TRAIN_BATCH_SIZE = 8",
    "    EVAL_BATCH_SIZE = 16",
    "    GRADIENT_ACCUMULATION_STEPS = 4",
    "    print(\"Using default configuration values.\")",
    "",
    "# Load the dataset with retry logic",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    \n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "    else:\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "        \n",
    "    print(\"Example features:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset example: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we're dealing with a code understanding task, we need to prepare our data appropriately. The dataset contains Swift code files, so we'll need to create labeled data for our task.\n",
    "\n",
    "For this demonstration, we'll create a binary classification task that determines whether the code is a Package.swift file (which is used for Swift package management) or not. This is just an example task - in a real application, you might have more complex classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "try:\n",
    "    # Apply the labeling function\n",
    "    labeled_data = data['train'].map(add_labels)\n",
    "    \n",
    "    # Check the distribution of labels using collections.Counter\n",
    "    import collections\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "        \n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "    max_label_count = max(label_counter.values())\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "Now let's split our data into training and validation sets with stratification to maintain label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert the label column to a ClassLabel type for stratification\n",
    "    from datasets import ClassLabel\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(set(labeled_data[\"label\"]))\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Create a new dataset with ClassLabel feature\n",
    "    labeled_data = labeled_data.cast_column(\"label\", ClassLabel(num_classes=num_labels, names=[str(i) for i in unique_labels]))\n",
    "    \n",
    "    # Split the dataset with stratification to maintain label distribution\n",
    "    train_test_split = labeled_data.train_test_split(test_size=0.1, seed=42, stratify_by_column='label')\n",
    "    train_data = train_test_split['train']\n",
    "    val_data = train_test_split['test']\n",
    "    \n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "    \n",
    "    # Check if dataset is large (might cause memory issues)\n",
    "    if len(train_data) > 10000:\n",
    "        print(\"\\nWARNING: You are training on a large dataset.\")\n",
    "        print(\"This may require significant memory, especially when using a GPU.\")\n",
    "        print(\"Consider reducing batch size or using gradient accumulation if you encounter memory issues.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we need to tokenize our code samples. We'll use the CodeBERT tokenizer to convert the Swift code into token IDs that the model can understand. We'll fix the inefficient tokenization by removing the `return_tensors=\"pt\"` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e349-613c-4353-9896-856f15daf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):",
    "    \"\"\"Tokenize the Swift code samples.",
    "    ",
    "    Args:",
    "        examples: Batch of examples from the dataset",
    "        ",
    "    Returns:",
    "        Tokenized examples",
    "    \"\"\"",
    "    # Tokenize the code content - FIXED: removed return_tensors=\"pt\" for memory efficiency",
    "    return tokenizer(",
    "        examples[\"content\"],",
    "        padding=False  # No padding during preprocessing saves memory,",
    "        truncation=True,",
    "        max_length=MAX_LENGTH  # CodeBERT supports sequences up to 512 tokens",
    "    ,",
    "        stride=128,      # Use stride for overlapping tokens when truncating long sequences",
    "        return_overflowing_tokens=False,  # Don't create new examples for long sequences",
    "        return_offsets_mapping=False,     # Don't return character mappings to save memory",
    "        return_special_tokens_mask=False  # Don't return special token mask to save memory",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:",
    "    # Determine the number of CPU cores available for parallel processing",
    "    import multiprocessing",
    "    # Use 75% of available CPUs for processing to avoid system slowdown",
    "    num_cpus = multiprocessing.cpu_count()",
    "    num_proc = max(1, int(num_cpus * 0.75))",
    "    print(f\"Using {num_proc} CPU cores for parallel processing\")",
    "    ",
    "    # Process the data with progress bars",
    "    tokenized_train_data = train_data.map(",
    "        tokenize_function,",
    "        batched=True,",
    "        batch_size=32,  # Reduced for lower memory usage, process 32 examples at a time for faster processing",
    "        num_proc=num_proc,  # Use multiple CPU cores for parallel processing",
    "        remove_columns=[col for col in train_data.column_names if col != 'label'],",
    "        desc=\"Tokenizing training data\",  # This adds a progress bar",
    "        load_from_cache_file=True,  # Use caching to speed up repeated runs",
    "        writer_batch_size=1000,  # Larger writer batch size for faster disk writes",
    "        new_fingerprint=f\"tokenized_train_{int(time.time())}\"  # Force cache update if code changes",
    "    )",
    "    ",
    "    tokenized_val_data = val_data.map(",
    "        tokenize_function,",
    "        batched=True,",
    "        batch_size=32,  # Reduced for lower memory usage, process 32 examples at a time for faster processing",
    "        num_proc=num_proc,  # Use multiple CPU cores for parallel processing",
    "        remove_columns=[col for col in val_data.column_names if col != 'label'],",
    "        desc=\"Tokenizing validation data\",  # This adds a progress bar",
    "        load_from_cache_file=True,  # Use caching to speed up repeated runs",
    "        writer_batch_size=1000,  # Larger writer batch size for faster disk writes",
    "        new_fingerprint=f\"tokenized_val_{int(time.time())}\"  # Force cache update if code changes",
    "    )",
    "    ",
    "    # Clean up memory",
    "    del train_data",
    "    del val_data",
    "    cleanup_memory()",
    "    ",
    "except Exception as e:",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Now that our data is ready, let's load the CodeBERT model and configure it for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:",
    "    # Load the CodeBERT model for sequence classification (2 classes)",
    "    model = AutoModelForSequenceClassification.from_pretrained(",
    "        MODEL_ID, ",
    "        num_labels=2,",
    "        low_cpu_mem_usage=True  # For memory efficiency",
    "    )",
    "    ",
    "    # Enable gradient checkpointing for better memory efficiency",
    "    try:",
    "        model.gradient_checkpointing_enable()",
    "        print(\"Gradient checkpointing enabled for memory efficiency.\")",
    "    except Exception as e:",
    "        print(f\"Could not enable gradient checkpointing: {e}\")",
    "        print(\"Will train without gradient checkpointing.\")",
    "    ",
    "    # Move model to the appropriate device",
    "    if not use_tpu:  # For TPU, the Trainer will handle device placement",
    "        model.to(device)",
    "        ",
    "    print(f\"Model type: {model.__class__.__name__}\")",
    "    ",
    "    # Calculate class weights for imbalanced dataset",
    "    label_counts = collections.Counter(train_data['label'])",
    "    total_samples = len(train_data)",
    "    class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}",
    "    print(f\"Class weights for handling imbalance: {class_weights}\")",
    "except Exception as e:",
    "    print(f\"Error loading model: {e}\")",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics during evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    try:\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Calculate multiple metrics\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Calculate per-class metrics for better understanding\n",
    "        per_class_precision, per_class_recall, per_class_f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average=None\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for i, (p, r, f) in enumerate(zip(per_class_precision, per_class_recall, per_class_f1)):\n",
    "            result[f'precision_class_{i}'] = p\n",
    "            result[f'recall_class_{i}'] = r\n",
    "            result[f'f1_class_{i}'] = f\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for dynamic padding",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)",
    "",
    "# Define training arguments with TPU support",
    "try:",
    "    # Set up training arguments with optimizations for faster and more efficient training",
    "    # Modified for older versions of transformers",
    "    training_args = TrainingArguments(",
    "        output_dir=\"./results/codebert-swift\",",
    "        # Removed newer parameters that aren't supported in older versions",
    "        # No evaluation_strategy, eval_steps, or save_strategy",
    "        save_steps=100,               # Save every 100 steps",
    "        save_total_limit=3,           # Keep only the 3 best checkpoints",
    "        learning_rate=5e-5,",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,  # Reduced batch size for memory efficiency,",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Critical for memory efficiency,",
    "        num_train_epochs=2,  # Reduced to 2 epochs for faster training,",
    "        weight_decay=0.01,",
    "        # Removed warmup_ratio in favor of warmup_steps",
    "        logging_dir=\"./logs\",",
    "        logging_steps=50,",
    "        # Removed newer parameters: load_best_model_at_end, metric_for_best_model, push_to_hub, optim",
    "        # TPU-specific configurations",
    "        tpu_num_cores=8 if use_tpu else None,  # 8 cores for TPU v2/v3",
    "        dataloader_drop_last=True if use_tpu else False,  # Important for TPU",
    "        # Memory optimizations",
    "        fp16=use_gpu,                 # Use mixed precision on GPU",
    "        # Removed bf16 parameter",
    "        dataloader_num_workers=2,  # Reduced for less memory overhead, fewer workers for less memory usage",
    "        dataloader_pin_memory=True,   # Pin memory for faster data transfer to GPU",
    "        # Removed gradient_checkpointing",
    "        max_grad_norm=1.0,           # Clip gradients to prevent exploding gradients",
    "        # Removed early_stopping_patience",
    "        # Optimizer settings",
    "        adam_beta1=0.9,",
    "        adam_beta2=0.999,",
    "        adam_epsilon=1e-8,",
    "        # Removed report_to parameter",
    "    )",
    "    ",
    "    print(\"Training arguments configured successfully.\")",
    "    print(f\"Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")",
    "except Exception as e:",
    "    print(f\"Error configuring training arguments: {e}\")",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer without the EarlyStoppingCallback\n",
    "try:\n",
    "    # Removed EarlyStoppingCallback because it requires metric_for_best_model parameter\n",
    "    # which isn't compatible with older versions of transformers\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_val_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,  # Added data collator for dynamic padding\n",
    "        # No callbacks - removed EarlyStoppingCallback to fix training error\n",
    "    )\n",
    "    \n",
    "    print(\"Trainer initialized successfully without early stopping.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating trainer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-recovery",
   "metadata": {},
   "source": [
    "## Checkpoint Recovery\n",
    "\n",
    "Let's add checkpoint recovery logic to resume training if it was interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-for-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints\n",
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(output_dir):\n",
    "            return None\n",
    "            \n",
    "        checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "            \n",
    "        # Extract checkpoint numbers and find the latest\n",
    "        checkpoint_nums = [int(c.split(\"-\")[1]) for c in checkpoints]\n",
    "        latest_checkpoint = max(checkpoint_nums)\n",
    "        return os.path.join(output_dir, f\"checkpoint-{latest_checkpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding latest checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check for existing checkpoint\n",
    "latest_checkpoint = find_latest_checkpoint(training_args.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Found existing checkpoint at {latest_checkpoint}. Training will resume from this point.\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found. Training will start from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our CodeBERT model for Swift code classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with checkpoint recovery",
    "try:",
    "    # Clean up memory before training",
    "    cleanup_memory()",
    "    print(\"Starting model training...\")",
    "    train_result = trainer.train(resume_from_checkpoint=latest_checkpoint)",
    "    print(f\"Training completed. Metrics: {train_result.metrics}\")",
    "    ",
    "    # Save the final model",
    "    trainer.save_model()",
    "    print(\"Final model saved.\")",
    "    ",
    "    # Save training metrics",
    "    trainer.log_metrics(\"train\", train_result.metrics)",
    "    trainer.save_metrics(\"train\", train_result.metrics)",
    "    trainer.save_state()",
    "except RuntimeError as e:",
    "    # Handle memory-related errors specially",
    "    error_msg = str(e)",
    "    print(f\"Runtime error during training: {error_msg}\")",
    "    ",
    "    if \"memory\" in error_msg.lower() or \"cuda out of memory\" in error_msg.lower() or \"resource exhausted\" in error_msg.lower():",
    "        print(\"",
    "MEMORY ERROR DETECTED! Try further reducing these parameters:\")",
    "        print(f\"1. Reduce MAX_LENGTH (currently {MAX_LENGTH}). Try 256 or 192.\")",
    "        print(f\"2. Reduce TRAIN_BATCH_SIZE (currently {TRAIN_BATCH_SIZE}). Try 8 or 4.\")",
    "        print(f\"3. Increase GRADIENT_ACCUMULATION_STEPS (currently {GRADIENT_ACCUMULATION_STEPS}). Try 8 or 16.\")",
    "    ",
    "    # Try to save the current state if possible",
    "    try:",
    "        print(\"Attempting to save current model state...\")",
    "        trainer.save_model(\"./results/codebert-swift-emergency-save\")",
    "        print(\"Emergency model save completed.\")",
    "    except Exception as save_err:",
    "        print(f\"Could not perform emergency save: {save_err}\")",
    "    ",
    "    # Re-raise the exception for proper error handling",
    "    raise",
    "except Exception as e:",
    "    print(f\"Error during training: {e}\")",
    "    # Try to save the current state if possible",
    "    try:",
    "        # Clean up memory before training",
    "        cleanup_memory()",
    "        print(\"Attempting to save current model state...\")",
    "        trainer.save_model(\"./results/codebert-swift-emergency-save\")",
    "        print(\"Emergency model save completed.\")",
    "    except:",
    "        print(\"Could not perform emergency save.\")",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our model on the validation dataset with improved sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with improved sampling",
    "try:",
    "    # Clean up memory before evaluation",
    "    cleanup_memory()",
    "    print(\"Evaluating model on validation dataset...\")",
    "    eval_results = trainer.evaluate()",
    "    print(f\"Evaluation results: {eval_results}\")",
    "    ",
    "    # Save evaluation metrics",
    "    trainer.log_metrics(\"eval\", eval_results)",
    "    trainer.save_metrics(\"eval\", eval_results)",
    "    ",
    "    # Evaluate on the entire training dataset to check for overfitting",
    "    print(\"\\nEvaluating model on training dataset to check for overfitting...\")",
    "    train_eval_results = trainer.evaluate(tokenized_train_data)",
    "    print(f\"Training evaluation results: {train_eval_results}\")",
    "    ",
    "    # Check for overfitting",
    "    train_f1 = train_eval_results.get('eval_f1', 0)",
    "    val_f1 = eval_results.get('eval_f1', 0)",
    "    if train_f1 - val_f1 > 0.1:",
    "        print(f\"WARNING: Possible overfitting detected. Training F1: {train_f1:.4f}, Validation F1: {val_f1:.4f}\")",
    "except Exception as e:",
    "    print(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-section",
   "metadata": {},
   "source": [
    "## Testing the Model with Example Predictions\n",
    "\n",
    "Let's test our model on some sample Swift code files with improved error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples with better sampling\n",
    "try:\n",
    "    # Sample more examples for better evaluation\n",
    "    num_samples = min(20, len(val_data))  # Increased from 5 to 20 or max available\n",
    "    \n",
    "    # Stratified sampling to ensure we get examples from each class\n",
    "    class_0_indices = [i for i, label in enumerate(val_data['label']) if label == 0]\n",
    "    class_1_indices = [i for i, label in enumerate(val_data['label']) if label == 1]\n",
    "    \n",
    "    # Sample from each class\n",
    "    samples_per_class = num_samples // 2\n",
    "    class_0_samples = random.sample(class_0_indices, min(samples_per_class, len(class_0_indices)))\n",
    "    class_1_samples = random.sample(class_1_indices, min(samples_per_class, len(class_1_indices)))\n",
    "    \n",
    "    # Combine samples\n",
    "    sample_indices = class_0_samples + class_1_samples\n",
    "    test_examples = val_data.select(sample_indices)\n",
    "    \n",
    "    # Tokenize them\n",
    "    tokenized_test_examples = tokenizer(\n",
    "        test_examples[\"content\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    for key, val in tokenized_test_examples.items():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            tokenized_test_examples[key] = val.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v for k, v in tokenized_test_examples.items() if k != \"label\"})\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "    \n",
    "    # Print results\n",
    "    correct_predictions = 0\n",
    "    for i, (pred, true) in enumerate(zip(predicted_labels, test_examples[\"label\"])):\n",
    "        is_package_swift = \"Yes\" if pred == 1 else \"No\"\n",
    "        true_is_package_swift = \"Yes\" if true == 1 else \"No\"\n",
    "        is_correct = pred == true\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"File path: {test_examples['path'][i]}\")\n",
    "        print(f\"Prediction: Is Package.swift? {is_package_swift} (Confidence: {predictions[i][pred].item():.4f})\")\n",
    "        print(f\"True label: Is Package.swift? {true_is_package_swift}\")\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "        print(f\"First few lines: {test_examples['content'][i][:100]}...\")\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    # Print overall accuracy on these examples\n",
    "    accuracy = correct_predictions / len(predicted_labels)\n",
    "    print(f\"Accuracy on these {len(predicted_labels)} examples: {accuracy:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-section",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Now let's save the model and tokenizer for later use with improved error handling and verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a directory for the model\n",
    "    model_save_dir = \"./codebert-swift-model\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if directory already contains model files\n",
    "    existing_files = os.listdir(model_save_dir) if os.path.exists(model_save_dir) else []\n",
    "    if existing_files:\n",
    "        print(f\"WARNING: Directory {model_save_dir} already contains files: {existing_files}\")\n",
    "        print(\"Creating a timestamped directory to avoid overwriting...\")\n",
    "        import datetime\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_save_dir = f\"./codebert-swift-model_{timestamp}\"\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to {model_save_dir}...\")\n",
    "    model.save_pretrained(model_save_dir)\n",
    "    tokenizer.save_pretrained(model_save_dir)\n",
    "    \n",
    "    # Save training arguments and configuration\n",
    "    with open(os.path.join(model_save_dir, \"training_args.json\"), \"w\") as f:\n",
    "        json.dump(training_args.to_dict(), f, indent=2)\n",
    "    \n",
    "    # Verify the saved files\n",
    "    expected_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\"]\n",
    "    missing_files = [f for f in expected_files if not os.path.exists(os.path.join(model_save_dir, f))]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"WARNING: Some expected model files are missing: {missing_files}\")\n",
    "    else:\n",
    "        print(f\"Model and tokenizer saved successfully to {model_save_dir}\")\n",
    "        \n",
    "    # Create a zip file for easier distribution\n",
    "    import shutil\n",
    "    zip_path = f\"{model_save_dir}.zip\"\n",
    "    print(f\"Creating zip archive at {zip_path}...\")\n",
    "    shutil.make_archive(model_save_dir, 'zip', os.path.dirname(model_save_dir), os.path.basename(model_save_dir))\n",
    "    print(f\"Zip archive created successfully at {zip_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropbox-section",
   "metadata": {},
   "source": [
    "## Uploading to Dropbox\n",
    "\n",
    "Now let's upload our trained model to Dropbox for easy access and distribution with improved error handling and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropbox-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Dropbox credentials\n",
    "# You should set these as environment variables in a production environment\n",
    "APP_KEY = \"your_app_key\"  # Replace with your actual app key\n",
    "APP_SECRET = \"your_app_secret\"  # Replace with your actual app secret\n",
    "REFRESH_TOKEN = \"your_refresh_token\"  # Replace with your actual refresh token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "from dropbox.exceptions import ApiError\n",
    "from dropbox.files import WriteMode\n",
    "\n",
    "def validate_dropbox_credentials(app_key, app_secret, refresh_token):\n",
    "    \"\"\"Test Dropbox credentials before attempting upload.\"\"\"\n",
    "    try:\n",
    "        print(\"Validating Dropbox credentials...\")\n",
    "        dbx = dropbox.Dropbox(\n",
    "            app_key=app_key,\n",
    "            app_secret=app_secret,\n",
    "            oauth2_refresh_token=refresh_token\n",
    "        )\n",
    "        # Check that the access token is valid\n",
    "        account = dbx.users_get_current_account()\n",
    "        print(f\"\u2705 Connected to Dropbox account: {account.name.display_name}\")\n",
    "        return True, dbx\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error connecting to Dropbox: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Validate Dropbox credentials\n",
    "credentials_valid, dbx = validate_dropbox_credentials(APP_KEY, APP_SECRET, REFRESH_TOKEN)\n",
    "if not credentials_valid:\n",
    "    print(\"Please check your Dropbox credentials and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dropbox(file_path, dropbox_path, max_retries=3):\n",
    "    \"\"\"Upload a file to Dropbox with retry logic.\"\"\"\n",
    "    if not credentials_valid:\n",
    "        print(\"Dropbox credentials are not valid. Cannot upload.\")\n",
    "        return False\n",
    "        \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                # For small files, upload in one go\n",
    "                if file_size <= chunk_size:\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path}...\")\n",
    "                    try:\n",
    "                        dbx.files_upload(f.read(), dropbox_path, mode=WriteMode('overwrite'))\n",
    "                        print(\"Upload complete!\")\n",
    "                        return True\n",
    "                    except ApiError as e:\n",
    "                        print(f\"ERROR: Dropbox API error - {e}\")\n",
    "                        if attempt < max_retries - 1:\n",
    "                            print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "                            continue\n",
    "                        return False\n",
    "                \n",
    "                # For large files, use chunked upload\n",
    "                else:\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path} in chunks...\")\n",
    "                    upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "                    cursor = dropbox.files.UploadSessionCursor(\n",
    "                        session_id=upload_session_start_result.session_id,\n",
    "                        offset=f.tell()\n",
    "                    )\n",
    "                    commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode('overwrite'))\n",
    "                    \n",
    "                    # Upload the file in chunks with progress tracking\n",
    "                    uploaded = f.tell()\n",
    "                    with tqdm(total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True) as pbar:\n",
    "                        pbar.update(uploaded)\n",
    "                        while uploaded < file_size:\n",
    "                            if (file_size - uploaded) <= chunk_size:\n",
    "                                dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "                                uploaded = file_size\n",
    "                                pbar.update(file_size - pbar.n)\n",
    "                            else:\n",
    "                                dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "                                uploaded = f.tell()\n",
    "                                cursor.offset = uploaded\n",
    "                                pbar.update(chunk_size)\n",
    "                    print(\"Chunked upload complete!\")\n",
    "                    return True\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Upload failed - {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Upload failed.\")\n",
    "                return False\n",
    "    return False\n",
    "\n",
    "def create_shared_link(dropbox_path):\n",
    "    \"\"\"Create a shared link for a file in Dropbox.\"\"\"\n",
    "    if not credentials_valid:\n",
    "        print(\"Dropbox credentials are not valid. Cannot create shared link.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
    "        return shared_link.url\n",
    "    except ApiError as e:\n",
    "        # If the file already has a shared link, the API will return an error\n",
    "        if isinstance(e.error, dropbox.sharing.CreateSharedLinkWithSettingsError) and \\\n",
    "           e.error.is_path() and e.error.get_path().is_shared_link_already_exists():\n",
    "            # Get existing shared links\n",
    "            shared_links = dbx.sharing_list_shared_links(path=dropbox_path).links\n",
    "            if shared_links:\n",
    "                return shared_links[0].url\n",
    "        print(f\"ERROR: Could not create shared link - {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model zip to Dropbox\n",
    "if credentials_valid:\n",
    "    zip_path = f\"{model_save_dir}.zip\"\n",
    "    dropbox_path = f\"/codebert-swift-model/{os.path.basename(zip_path)}\"\n",
    "    \n",
    "    if upload_to_dropbox(zip_path, dropbox_path):\n",
    "        print(f\"Successfully uploaded model to Dropbox at {dropbox_path}\")\n",
    "        shared_link = create_shared_link(dropbox_path)\n",
    "        if shared_link:\n",
    "            print(f\"Shared link: {shared_link}\")\n",
    "    else:\n",
    "        print(\"Failed to upload model to Dropbox.\")\n",
    "else:\n",
    "    print(\"Skipping Dropbox upload due to invalid credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Set up our environment with proper accelerator detection (CPU, GPU, or TPU)\n",
    "2. Loaded and preprocessed the Swift code dataset with error handling and validation\n",
    "3. Fine-tuned the CodeBERT model for Swift code classification with optimized training parameters\n",
    "4. Evaluated the model's performance with comprehensive metrics\n",
    "5. Saved and uploaded the model to Dropbox for easy access\n",
    "\n",
    "The model can now be used for Swift code understanding tasks, such as identifying Package.swift files. This is just one example of how CodeBERT can be fine-tuned for code-related tasks. The same approach can be extended to other programming languages and tasks, such as code search, code completion, and bug detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}