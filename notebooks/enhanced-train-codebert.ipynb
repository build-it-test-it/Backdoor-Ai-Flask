{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding\n",
    "\n",
    "In this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n",
    "\n",
    "We'll use the Swift code dataset to fine-tune the model for code understanding tasks. After training, we'll upload the model to Dropbox for easy access and distribution.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The process of fine-tuning CodeBERT involves:\n",
    "\n",
    "1. **ðŸ”§ Setup**: Install necessary libraries and prepare our environment\n",
    "2. **ðŸ“¥ Data Loading**: Load the Swift code dataset from Hugging Face\n",
    "3. **ðŸ§¹ Preprocessing**: Prepare the data for training by tokenizing the code samples\n",
    "4. **ðŸ§  Model Training**: Fine-tune CodeBERT on our prepared data with optimized performance\n",
    "5. **ðŸ“Š Evaluation**: Assess how well our model performs\n",
    "6. **ðŸ“¤ Export & Upload**: Save the model and upload it to Dropbox\n",
    "\n",
    "This enhanced version includes optimizations for faster and more efficient training.\n",
    "\n",
    "Let's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    default_data_collator,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# For memory optimization\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU) with enhanced detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and configure accelerator with better error handling\n",
    "def detect_and_configure_accelerator():\n",
    "    \"\"\"Detect and configure the available accelerator (CPU, GPU, or TPU) with enhanced detection.\"\"\"\n",
    "    try:\n",
    "        # Initialize accelerator from HF accelerate library\n",
    "        accelerator = Accelerator()\n",
    "        if accelerator.distributed_type == \"TPU\":\n",
    "            print(\"TPU detected! Configuring for TPU training...\")\n",
    "            device = accelerator.device\n",
    "            use_tpu = True\n",
    "            use_gpu = False\n",
    "            use_mixed_precision = True\n",
    "            return device, use_tpu, use_gpu, use_mixed_precision, accelerator\n",
    "        \n",
    "        # Check for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU detected! Using {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            use_tpu = False\n",
    "            use_gpu = True\n",
    "            use_mixed_precision = True  # Enable mixed precision by default for GPU\n",
    "            print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            \n",
    "            # Clear GPU cache to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(\"No GPU or TPU detected. Using CPU (this will be slow).\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            use_tpu = False\n",
    "            use_gpu = False\n",
    "            use_mixed_precision = False  # Disable mixed precision for CPU\n",
    "        \n",
    "        return device, use_tpu, use_gpu, use_mixed_precision, accelerator\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting accelerator: {e}\")\n",
    "        print(\"Defaulting to CPU.\")\n",
    "        return torch.device(\"cpu\"), False, False, False, None\n",
    "\n",
    "# Detect and configure accelerator\n",
    "device, use_tpu, use_gpu, use_mixed_precision, accelerator = detect_and_configure_accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using with optimized batch sizes and memory settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and dataset IDs\n",
    "MODEL_ID = \"microsoft/codebert-base\"\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Configure batch sizes based on available hardware with optimized values\n",
    "if use_tpu:\n",
    "    TRAIN_BATCH_SIZE = 64  # Larger batch size for TPU\n",
    "    EVAL_BATCH_SIZE = 128\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    NUM_WORKERS = 8\n",
    "elif use_gpu:\n",
    "    # Dynamically adjust batch size based on available GPU memory\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory_gb > 16:  # High-end GPU\n",
    "        TRAIN_BATCH_SIZE = 32\n",
    "        EVAL_BATCH_SIZE = 64\n",
    "        GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    elif gpu_memory_gb > 8:  # Mid-range GPU\n",
    "        TRAIN_BATCH_SIZE = 16\n",
    "        EVAL_BATCH_SIZE = 32\n",
    "        GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    else:  # Low-end GPU\n",
    "        TRAIN_BATCH_SIZE = 8\n",
    "        EVAL_BATCH_SIZE = 16\n",
    "        GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    NUM_WORKERS = min(4, os.cpu_count() or 1)\n",
    "else:\n",
    "    TRAIN_BATCH_SIZE = 4   # Smaller batch size for CPU\n",
    "    EVAL_BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    NUM_WORKERS = 0  # No multiprocessing on CPU\n",
    "\n",
    "# Set maximum sequence length for tokenization\n",
    "MAX_SEQ_LENGTH = 512  # CodeBERT's maximum sequence length\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Training batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Evaluation batch size: {EVAL_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Number of dataloader workers: {NUM_WORKERS}\")\n",
    "print(f\"Using mixed precision: {use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling and caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic and caching\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic and caching.\"\"\"\n",
    "    cache_dir = os.path.join(os.getcwd(), \"dataset_cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True, cache_dir=cache_dir)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    \n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "    else:\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "        \n",
    "    print(\"Example features:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset example: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling and caching\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)  # Use fast tokenizer for better performance\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we're dealing with a code understanding task, we need to prepare our data appropriately. The dataset contains Swift code files, so we'll need to create labeled data for our task.\n",
    "\n",
    "For this demonstration, we'll create a binary classification task that determines whether the code is a Package.swift file (which is used for Swift package management) or not. This is just an example task - in a real application, you might have more complex classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "try:\n",
    "    # Apply the labeling function\n",
    "    labeled_data = data['train'].map(add_labels)\n",
    "    \n",
    "    # Check the distribution of labels using collections.Counter\n",
    "    import collections\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "        \n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "    max_label_count = max(label_counter.values())\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Now let's split our data into training and validation sets. We'll use scikit-learn's train_test_split to avoid the ClassLabel issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert to pandas DataFrame for easier manipulation\n",
    "    df = labeled_data.to_pandas()\n",
    "    \n",
    "    # Split using scikit-learn's train_test_split with stratification\n",
    "    train_df, val_df = sklearn_train_test_split(\n",
    "        df, \n",
    "        test_size=0.1, \n",
    "        random_state=42, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Convert back to HuggingFace datasets\n",
    "    train_data = Dataset.from_pandas(train_df)\n",
    "    val_data = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "    \n",
    "    # Check if dataset is large (might cause memory issues)\n",
    "    if len(train_data) > 10000:\n",
    "        print(\"\\nWARNING: You are training on a large dataset.\")\n",
    "        print(\"This may require significant memory, especially when using a GPU.\")\n",
    "        print(\"Consider reducing batch size or using gradient accumulation if you encounter memory issues.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9",
   "metadata": {},
   "source": [
    "## Optimized Tokenization\n",
    "\n",
    "Now we need to tokenize our code samples. We'll use the CodeBERT tokenizer to convert the Swift code into token IDs that the model can understand. This implementation is optimized for speed and memory efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e349-613c-4353-9896-856f15daf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the Swift code samples with optimized settings.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized examples\n",
    "    \"\"\"\n",
    "    # Tokenize the code content with optimized settings\n",
    "    # - No return_tensors=\"pt\" for memory efficiency\n",
    "    # - padding=False for dynamic padding later with DataCollator\n",
    "    # - truncation=True to handle long sequences\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        padding=False,  # We'll use dynamic padding with DataCollator\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process the data with progress bars and optimized settings\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in larger batches for speed\n",
    "        remove_columns=[col for col in train_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing training data\",  # This adds a progress bar\n",
    "        num_proc=NUM_WORKERS if NUM_WORKERS > 0 else None  # Use multiprocessing if available\n",
    "    )\n",
    "    \n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in larger batches for speed\n",
    "        remove_columns=[col for col in val_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing validation data\",  # This adds a progress bar\n",
    "        num_proc=NUM_WORKERS if NUM_WORKERS > 0 else None  # Use multiprocessing if available\n",
    "    )\n",
    "    \n",
    "    # Set format for pytorch\n",
    "    tokenized_train_data = tokenized_train_data.with_format(\"torch\")\n",
    "    tokenized_val_data = tokenized_val_data.with_format(\"torch\")\n",
    "    \n",
    "    print(\"Training data after tokenization:\")\n",
    "    print(tokenized_train_data)\n",
    "    print(\"\\nValidation data after tokenization:\")\n",
    "    print(tokenized_val_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Now that our data is ready, let's load the CodeBERT model and configure it for sequence classification with memory optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the CodeBERT model for sequence classification (2 classes)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        num_labels=2,\n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        gradient_checkpointing=True if use_gpu or use_tpu else False\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if not use_tpu:  # For TPU, the Trainer will handle device placement\n",
    "        model.to(device)\n",
    "        \n",
    "    print(f\"Model type: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Calculate class weights for imbalanced dataset\n",
    "    label_counts = collections.Counter(train_data['label'])\n",
    "    total_samples = len(train_data)\n",
    "    class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}\n",
    "    print(f\"Class weights for handling imbalance: {class_weights}\")\n",
    "    \n",
    "    # Convert class weights to tensor for loss function\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [class_weights[i] for i in range(len(class_weights))],\n",
    "        dtype=torch.float\n",
    "    ).to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics with optimized settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics during evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    try:\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Calculate multiple metrics\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Calculate per-class metrics for better understanding\n",
    "        per_class_precision, per_class_recall, per_class_f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average=None\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for i, (p, r, f) in enumerate(zip(per_class_precision, per_class_recall, per_class_f1)):\n",
    "            result[f'precision_class_{i}'] = p\n",
    "            result[f'recall_class_{i}'] = r\n",
    "            result[f'f1_class_{i}'] = f\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments with optimized settings\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/codebert-swift\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,  # Keep only the 3 best checkpoints\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "        # TPU-specific configurations\n",
    "        tpu_num_cores=8 if use_tpu else None,  # 8 cores for TPU v2/v3\n",
    "        dataloader_drop_last=True if use_tpu else False,  # Important for TPU\n",
    "        # Memory and performance optimizations\n",
    "        fp16=use_mixed_precision,  # Use mixed precision when available\n",
    "        dataloader_num_workers=NUM_WORKERS,\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        max_grad_norm=1.0,\n",
    "        # Warmup steps for learning rate scheduler\n",
    "        warmup_ratio=0.1,  # Warm up over 10% of training steps\n",
    "        # Early stopping\n",
    "        # Reporting\n",
    "        report_to=[\"tensorboard\"],\n",
    "        # Optimizer settings\n",
    "        optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n",
    "        # Avoid OOM errors by not storing gradients for all steps\n",
    "        gradient_checkpointing=True if (use_gpu or use_tpu) else False,\n",
    "        # Avoid unnecessary memory usage\n",
    "        remove_unused_columns=True,\n",
    "        # Disable tqdm progress bars in favor of our own reporting\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Training arguments configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring training arguments: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer with data collator and callbacks\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_val_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,  # Added data collator for dynamic padding\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Added early stopping\n",
    "    )\n",
    "    \n",
    "    print(\"Trainer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating trainer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-recovery",
   "metadata": {},
   "source": [
    "## Checkpoint Recovery\n",
    "\n",
    "Let's add improved checkpoint recovery logic to resume training if it was interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-for-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints using HuggingFace's built-in function\n",
    "try:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use HuggingFace's get_last_checkpoint function\n",
    "    latest_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found existing checkpoint at {latest_checkpoint}. Training will resume from this point.\")\n",
    "    else:\n",
    "        print(\"No existing checkpoint found. Training will start from scratch.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for checkpoints: {e}\")\n",
    "    latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our CodeBERT model for Swift code classification with optimized settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with checkpoint recovery and memory optimization\n",
    "try:\n",
    "    # Clear memory before training\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    print(f\"Training completed. Metrics: {train_result.metrics}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model()\n",
    "    print(\"Final model saved.\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "try:\n",
    "    print(\"Evaluating model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    trainer.log_metrics(\"eval\", eval_results)\n",
    "    trainer.save_metrics(\"eval\", eval_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-export",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Now let's export our model for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to a specific directory\n",
    "try:\n",
    "    export_dir = \"./exported-model\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained(export_dir)\n",
    "    tokenizer.save_pretrained(export_dir)\n",
    "    \n",
    "    # Save model configuration and metadata\n",
    "    model_info = {\n",
    "        \"model_name\": \"CodeBERT-Swift\",\n",
    "        \"base_model\": MODEL_ID,\n",
    "        \"task\": \"binary_classification\",\n",
    "        \"labels\": [\"Not Package.swift\", \"Package.swift\"],\n",
    "        \"metrics\": eval_results,\n",
    "        \"training_params\": {\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"learning_rate\": training_args.learning_rate,\n",
    "            \"epochs\": training_args.num_train_epochs,\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "            \"training_samples\": len(tokenized_train_data),\n",
    "            \"validation_samples\": len(tokenized_val_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(export_dir, \"model_info.json\"), \"w\") as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "        \n",
    "    print(f\"Model exported to {export_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-upload",
   "metadata": {},
   "source": [
    "## Upload to Dropbox (Optional)\n",
    "\n",
    "If you want to upload the model to Dropbox for easy access, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload a file to Dropbox\n",
    "def upload_to_dropbox(file_path, dropbox_path, access_token):\n",
    "    \"\"\"Upload a file to Dropbox.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to upload\n",
    "        dropbox_path: Path in Dropbox where the file should be uploaded\n",
    "        access_token: Dropbox access token\n",
    "        \n",
    "    Returns:\n",
    "        Response from Dropbox API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import dropbox\n",
    "        dbx = dropbox.Dropbox(access_token)\n",
    "        \n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "            \n",
    "            if file_size <= chunk_size:\n",
    "                # Small file, upload in one go\n",
    "                return dbx.files_upload(f.read(), dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "            else:\n",
    "                # Large file, use chunked upload\n",
    "                upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "                cursor = dropbox.files.UploadSessionCursor(\n",
    "                    session_id=upload_session_start_result.session_id,\n",
    "                    offset=f.tell()\n",
    "                )\n",
    "                commit = dropbox.files.CommitInfo(path=dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "                \n",
    "                while f.tell() < file_size:\n",
    "                    if (file_size - f.tell()) <= chunk_size:\n",
    "                        # Last chunk\n",
    "                        return dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "                    else:\n",
    "                        # Intermediate chunk\n",
    "                        dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "                        cursor.offset = f.tell()\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Dropbox: {e}\")\n",
    "        raise\n",
    "\n",
    "# To use this function, uncomment and provide your Dropbox access token\n",
    "# DROPBOX_ACCESS_TOKEN = \"your_access_token_here\"\n",
    "# upload_to_dropbox(\"./exported-model.zip\", \"/CodeBERT-Swift/model.zip\", DROPBOX_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've fine-tuned the CodeBERT model on Swift code with optimized training settings. The model can now be used for code understanding tasks related to Swift.\n",
    "\n",
    "Key optimizations implemented:\n",
    "\n",
    "1. **Mixed precision training** for faster computation\n",
    "2. **Gradient checkpointing** to reduce memory usage\n",
    "3. **Dynamic batch sizing** based on available hardware\n",
    "4. **Efficient data loading** with multiprocessing\n",
    "5. **Memory management** with garbage collection and cache clearing\n",
    "6. **Optimized tokenization** with batched processing\n",
    "7. **Proper stratification** using scikit-learn instead of datasets library\n",
    "8. **Improved checkpoint handling** for reliable training resumption\n",
    "\n",
    "The model is now ready for deployment in your applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}