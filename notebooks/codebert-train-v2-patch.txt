# NOTEBOOK PATCH INSTRUCTIONS
#
# In the "Load the model" code cell, replace the "Extend position embeddings" section with:

# Extend position embeddings to support longer sequences
if MAX_LENGTH > 512:
    print(f"Extending position embeddings from {model.config.max_position_embeddings} to {MAX_LENGTH}")
    
    # Get current position embeddings max length
    current_max_pos = model.config.max_position_embeddings
    
    # Detect if we're using RoBERTa or BERT model
    is_roberta = hasattr(model, 'roberta')
    embeddings = model.roberta.embeddings if is_roberta else model.bert.embeddings
    
    # Initialize new position embeddings for the extended range
    new_pos_embed = embeddings.position_embeddings.weight.new_empty(MAX_LENGTH, model.config.hidden_size)
    
    # Copy existing weights
    new_pos_embed[:current_max_pos] = embeddings.position_embeddings.weight
    
    # Initialize remaining weights using Xavier initialization
    import torch.nn as nn
    nn.init.xavier_uniform_(new_pos_embed[current_max_pos:])
    
    # Update the model config
    model.config.max_position_embeddings = MAX_LENGTH
    
    # Replace the position embeddings weights
    embeddings.position_embeddings = nn.Embedding.from_pretrained(new_pos_embed, freeze=False)
    
    # Update the position_ids in the model to work with new length
    embeddings.register_buffer(
        "position_ids", torch.arange(MAX_LENGTH).expand((1, -1))
    )
    
    # For RoBERTa models, we need to ensure token_type_ids are properly handled
    if is_roberta and hasattr(embeddings, "token_type_ids"):
        # Create a new token_type_ids buffer with the expanded size
        embeddings.register_buffer(
            "token_type_ids",
            torch.zeros(1, MAX_LENGTH, dtype=torch.long),
            persistent=False,
        )
    
    print("Successfully extended position embeddings")
