{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# CodeBERT for Swift Code Understanding (Fixed Version)\n\nIn this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n\nWe'll use the Swift code dataset to fine-tune the model for code understanding tasks. After training, we'll upload the model to Dropbox for easy access and distribution.\n\n## Overview\n\nThe process of fine-tuning CodeBERT involves:\n\n1. **\ud83d\udd27 Setup**: Install necessary libraries and prepare our environment\n2. **\ud83d\udce5 Data Loading**: Load the Swift code dataset from Hugging Face\n3. **\ud83e\uddf9 Preprocessing**: Prepare the data for training by tokenizing the code samples\n4. **\ud83e\udde0 Model Training**: Fine-tune CodeBERT on our prepared data\n5. **\ud83d\udcca Evaluation**: Assess how well our model performs\n6. **\ud83d\udce4 Export & Upload**: Save the model and upload it to Dropbox\n\nLet's start by installing the necessary libraries:\n\n**Note:** This is a fixed version of the notebook with improved error handling, TPU detection, and dataset safety checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall TensorFlow and install TensorFlow-cpu (better for Kaggle environment)\n",
    "\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow-cpu\n",
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: These imports must be properly separated\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim instead of transformers.optimization\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Add memory management function\n",
    "def cleanup_memory():\n",
    "\n",
    "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Memory cleaned up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"microsoft/CodeXGLUE\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "print(\"Using default configuration values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "\n",
    "            return data\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "\n",
    "                raise\n",
    "\n",
    "# Make sure dataset ID is defined (in case previous cell didn't execute)if 'DATASET_ID' not in globals():    print(\"Warning: DATASET_ID not found. Using default value.\")    DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"  # Default value as fallback    MAX_LENGTH = 384    MODEL_ID = \"microsoft/codebert-base\"    TRAIN_BATCH_SIZE = 8    EVAL_BATCH_SIZE = 16    GRADIENT_ACCUMULATION_STEPS = 4    print(\"Using default configuration values.\")# Load the dataset with retry logictry:\n",
    "\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "\n",
    "    print(\"Dataset structure:\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "\n",
    "    if 'train' not in dataset:\n",
    "\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "\n",
    "    if missing_columns:\n",
    "\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "\n",
    "if not dataset_valid:\n",
    "\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "\n",
    "    if 'train' in data:\n",
    "\n",
    "        example = data['train'][0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "\n",
    "    print(\"Example features:\")\n",
    "\n",
    "    for key, value in example.items():\n",
    "\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error exploring dataset example: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling\n",
    "try:\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we're dealing with a code understanding task, we need to prepare our data appropriately. The dataset contains Swift code files, so we'll need to create labeled data for our task.\n",
    "\n",
    "For this demonstration, we'll create a binary classification task that determines whether the code is a Package.swift file (which is used for Swift package management) or not. This is just an example task - in a real application, you might have more complex classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "try:\n",
    "\n",
    "    # Apply the labeling function\n",
    "    labeled_data = data['train'].map(add_labels)\n",
    "\n",
    "    # Check the distribution of labels using collections.Counter\n",
    "    import collections\n",
    "\n",
    "    all_labels = labeled_data['label']\n",
    "\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "\n",
    "    print(\"Label distribution:\")\n",
    "\n",
    "    for label, count in label_counter.items():\n",
    "\n",
    "        print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "\n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "\n",
    "    max_label_count = max(label_counter.values())\n",
    "\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "\n",
    "    if imbalance_ratio > 10:\n",
    "\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "\n",
    "    elif imbalance_ratio > 3:\n",
    "\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "Now let's split our data into training and validation sets with stratification to maintain label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Convert the label column to a ClassLabel type for stratification\n",
    "    from datasets import ClassLabel\n",
    "\n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(set(labeled_data[\"label\"]))\n",
    "\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    # Create a new dataset with ClassLabel feature\n",
    "    labeled_data = labeled_data.cast_column(\"label\", ClassLabel(num_classes=num_labels, names=[str(i) for i in unique_labels]))\n",
    "\n",
    "    # Split the dataset with stratification to maintain label distribution\n",
    "    train_test_split = labeled_data.train_test_split(test_size=0.1, seed=42, stratify_by_column='label')\n",
    "\n",
    "    train_data = train_test_split['train']\n",
    "\n",
    "    val_data = train_test_split['test']\n",
    "\n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "\n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "\n",
    "    # Check if dataset is large (might cause memory issues)\n",
    "\n",
    "    if len(train_data) > 10000:\n",
    "\n",
    "        print(\"\\nWARNING: You are training on a large dataset.\")\n",
    "\n",
    "        print(\"This may require significant memory, especially when using a GPU.\")\n",
    "\n",
    "        print(\"Consider reducing batch size or using gradient accumulation if you encounter memory issues.\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we need to tokenize our code samples. We'll use the CodeBERT tokenizer to convert the Swift code into token IDs that the model can understand. We'll fix the inefficient tokenization by removing the `return_tensors=\"pt\"` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e349-613c-4353-9896-856f15daf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples['func'], \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the datasets\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_data.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "print(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "print(f\"Tokenized {len(tokenized_test_data)} test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    # Determine the number of CPU cores available for parallel processing    import multiprocessing    # Use 75% of available CPUs for processing to avoid system slowdown    num_cpus = multiprocessing.cpu_count()    num_proc = max(1, int(num_cpus * 0.75))    print(f\"Using {num_proc} CPU cores for parallel processing\")        # Process the data with progress bars    tokenized_train_data = train_data.map(        tokenize_function,        batched=True,        batch_size=32,  # Reduced for lower memory usage, process 32 examples at a time for faster processing        num_proc=num_proc,  # Use multiple CPU cores for parallel processing        remove_columns=[col for col in train_data.column_names if col != 'label'],        desc=\"Tokenizing training data\",  # This adds a progress bar        load_from_cache_file=True,  # Use caching to speed up repeated runs        writer_batch_size=1000,  # Larger writer batch size for faster disk writes        new_fingerprint=f\"tokenized_train_{int(time.time())}\"  # Force cache update if code changes    )        tokenized_val_data = val_data.map(        tokenize_function,        batched=True,        batch_size=32,  # Reduced for lower memory usage, process 32 examples at a time for faster processing        num_proc=num_proc,  # Use multiple CPU cores for parallel processing        remove_columns=[col for col in val_data.column_names if col != 'label'],        desc=\"Tokenizing validation data\",  # This adds a progress bar        load_from_cache_file=True,  # Use caching to speed up repeated runs        writer_batch_size=1000,  # Larger writer batch size for faster disk writes        new_fingerprint=f\"tokenized_val_{int(time.time())}\"  # Force cache update if code changes    )        # Clean up memory    del train_data    del val_data    cleanup_memory()    except Exception as e:    print(f\"Error during tokenization: {e}\")    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Now that our data is ready, let's load the CodeBERT model and configure it for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels = tokenized_train_data['label']\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels.numpy())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(np.unique(labels)),\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Define metrics computation function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for dynamic paddingdata_collator = DataCollatorWithPadding(tokenizer=tokenizer)# Define training arguments with TPU supporttry:    # Set up training arguments with optimizations for faster and more efficient training    training_args = TrainingArguments(        output_dir=\"./results/codebert-swift\",        save_steps=100,               # Save every 100 steps        save_total_limit=3,           # Keep only the 3 best checkpoints        learning_rate=5e-5,        per_device_train_batch_size=TRAIN_BATCH_SIZE,  # Reduced batch size for memory efficiency,        per_device_eval_batch_size=EVAL_BATCH_SIZE,        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Critical for memory efficiency        num_train_epochs=2,  # Reduced to 2 epochs for faster training        weight_decay=0.01,        logging_dir=\"./logs\",        logging_steps=50,        # TPU-specific configurations        tpu_num_cores=8 if use_tpu else None,  # 8 cores for TPU v2/v3        dataloader_drop_last=True if use_tpu else False,  # Important for TPU        # Memory optimizations        fp16=use_gpu,                 # Use mixed precision on GPU        dataloader_num_workers=2,     # Reduced for less memory overhead        dataloader_pin_memory=True,   # Pin memory for faster data transfer to GPU        max_grad_norm=1.0,            # Clip gradients to prevent exploding gradients        # Optimizer settings        adam_beta1=0.9,        adam_beta2=0.999,        adam_epsilon=1e-8    )        print(\"Training arguments configured successfully.\")    print(f\"Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")except Exception as e:    print(f\"Error configuring training arguments: {e}\")    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "try:\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_val_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,  # Added data collator for dynamic padding\n",
    "        # No callbacks - removed EarlyStoppingCallback to fix training error\n",
    "    )\n",
    "\n",
    "    print(\"Trainer initialized successfully without early stopping.\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error creating trainer: {e}\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-recovery",
   "metadata": {},
   "source": [
    "## Checkpoint Recovery\n",
    "\n",
    "Let's add checkpoint recovery logic to resume training if it was interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-for-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints\n",
    "def find_latest_checkpoint(output_dir):\n",
    "\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "\n",
    "            return None\n",
    "            \n",
    "        checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "\n",
    "        if not checkpoints:\n",
    "\n",
    "            return None\n",
    "            \n",
    "        # Extract checkpoint numbers and find the latest\n",
    "        checkpoint_nums = [int(c.split(\"-\")[1]) for c in checkpoints]\n",
    "\n",
    "        latest_checkpoint = max(checkpoint_nums)\n",
    "\n",
    "        return os.path.join(output_dir, f\"checkpoint-{latest_checkpoint}\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error finding latest checkpoint: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "# Check for existing checkpoint\n",
    "latest_checkpoint = find_latest_checkpoint(training_args.output_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "\n",
    "    print(f\"Found existing checkpoint at {latest_checkpoint}. Training will resume from this point.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"No existing checkpoint found. Training will start from scratch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our CodeBERT model for Swift code classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./codebert-finetuned\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our model on the validation dataset with improved sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test_data)\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Get predictions for test examples\n",
    "test_pred = trainer.predict(tokenized_test_data)\n",
    "test_preds = np.argmax(test_pred.predictions, axis=1)\n",
    "\n",
    "# Print some test examples with predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(min(5, len(test_data))):\n",
    "    example = test_data[i]\n",
    "    prediction = test_preds[i]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Code snippet: {example['func'][:100]}...\")\n",
    "    print(f\"True label: {example['label']}\")\n",
    "    print(f\"Predicted label: {prediction}\\n\")\n",
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_pred.label_ids, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-section",
   "metadata": {},
   "source": [
    "## Testing the Model with Example Predictions\n",
    "\n",
    "Let's test our model on some sample Swift code files with improved error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples with better sampling\n",
    "try:\n",
    "\n",
    "    # Sample more examples for better evaluation\n",
    "    num_samples = min(20, len(val_data))  # Increased from 5 to 20 or max available\n",
    "    \n",
    "    # Stratified sampling to ensure we get examples from each class\n",
    "    class_0_indices = [i for i, label in enumerate(val_data['label']) if label == 0]\n",
    "\n",
    "    class_1_indices = [i for i, label in enumerate(val_data['label']) if label == 1]\n",
    "\n",
    "    # Sample from each class\n",
    "    samples_per_class = num_samples // 2\n",
    "    class_0_samples = random.sample(class_0_indices, min(samples_per_class, len(class_0_indices)))\n",
    "\n",
    "    class_1_samples = random.sample(class_1_indices, min(samples_per_class, len(class_1_indices)))\n",
    "\n",
    "    # Combine samples\n",
    "    sample_indices = class_0_samples + class_1_samples\n",
    "    test_examples = val_data.select(sample_indices)\n",
    "\n",
    "    # Tokenize them\n",
    "    tokenized_test_examples = tokenizer(\n",
    "        test_examples[\"content\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,  # Use the same max length as in training\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move to device\n",
    "    for key, val in tokenized_test_examples.items():\n",
    "\n",
    "        if isinstance(val, torch.Tensor):\n",
    "\n",
    "            tokenized_test_examples[key] = val.to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(**{k: v for k, v in tokenized_test_examples.items() if k != \"label\"})\n",
    "\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        predicted_labels = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Print results\n",
    "    correct_predictions = 0\n",
    "    for i, (pred, true) in enumerate(zip(predicted_labels, test_examples[\"label\"])):\n",
    "\n",
    "        is_package_swift = \"Yes\" if pred == 1 else \"No\"\n",
    "        true_is_package_swift = \"Yes\" if true == 1 else \"No\"\n",
    "        is_correct = pred == true\n",
    "        if is_correct:\n",
    "\n",
    "            correct_predictions += 1\n",
    "            \n",
    "        print(f\"Example {i+1}:\")\n",
    "\n",
    "        print(f\"File path: {test_examples['path'][i]}\")\n",
    "\n",
    "        print(f\"Prediction: Is Package.swift? {is_package_swift} (Confidence: {predictions[i][pred].item():.4f})\")\n",
    "\n",
    "        print(f\"True label: Is Package.swift? {true_is_package_swift}\")\n",
    "\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "\n",
    "        print(f\"First few lines: {test_examples['content'][i][:100]}...\")\n",
    "\n",
    "        print(\"---\\n\")\n",
    "\n",
    "    # Print overall accuracy on these examples\n",
    "    accuracy = correct_predictions / len(predicted_labels)\n",
    "\n",
    "    print(f\"Accuracy on these {len(predicted_labels)} examples: {accuracy:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error testing model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-section",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Now let's save the model and tokenizer for later use with improved error handling and verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Create a directory for the model\n",
    "    model_save_dir = \"./codebert-swift-model\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    # Check if directory already contains model files\n",
    "    existing_files = os.listdir(model_save_dir) if os.path.exists(model_save_dir) else []\n",
    "\n",
    "    if existing_files:\n",
    "\n",
    "        print(f\"WARNING: Directory {model_save_dir} already contains files: {existing_files}\")\n",
    "\n",
    "        print(\"Creating a timestamped directory to avoid overwriting...\")\n",
    "\n",
    "        import datetime\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        model_save_dir = f\"./codebert-swift-model_{timestamp}\"\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Saving model to {model_save_dir}...\")\n",
    "\n",
    "    model.save_pretrained(model_save_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "    # Save training arguments and configuration\n",
    "    with open(os.path.join(model_save_dir, \"training_args.json\"), \"w\") as f:\n",
    "\n",
    "        json.dump(training_args.to_dict(), f, indent=2)\n",
    "\n",
    "    # Verify the saved files\n",
    "    expected_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\"]\n",
    "\n",
    "    missing_files = [f for f in expected_files if not os.path.exists(os.path.join(model_save_dir, f))]\n",
    "\n",
    "    if missing_files:\n",
    "\n",
    "        print(f\"WARNING: Some expected model files are missing: {missing_files}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"Model and tokenizer saved successfully to {model_save_dir}\")\n",
    "\n",
    "    # Create a zip file for easier distribution\n",
    "    import shutil\n",
    "\n",
    "    zip_path = f\"{model_save_dir}.zip\"\n",
    "    print(f\"Creating zip archive at {zip_path}...\")\n",
    "\n",
    "    shutil.make_archive(model_save_dir, 'zip', os.path.dirname(model_save_dir), os.path.basename(model_save_dir))\n",
    "\n",
    "    print(f\"Zip archive created successfully at {zip_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropbox-section",
   "metadata": {},
   "source": [
    "## Uploading to Dropbox\n",
    "\n",
    "Now let's upload our trained model to Dropbox for easy access and distribution with improved error handling and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropbox-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Dropbox credentials\n",
    "# You should set these as environment variables in a production environment\n",
    "APP_KEY = \"your_app_key\"  # Replace with your actual app key\n",
    "APP_SECRET = \"your_app_secret\"  # Replace with your actual app secret\n",
    "REFRESH_TOKEN = \"your_refresh_token\"  # Replace with your actual refresh token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "\n",
    "from dropbox.exceptions import ApiError\n",
    "\n",
    "from dropbox.files import WriteMode\n",
    "\n",
    "def validate_dropbox_credentials(app_key, app_secret, refresh_token):\n",
    "\n",
    "    \"\"\"Test Dropbox credentials before attempting upload.\"\"\"\n",
    "    try:\n",
    "\n",
    "        print(\"Validating Dropbox credentials...\")\n",
    "\n",
    "        dbx = dropbox.Dropbox(\n",
    "            app_key=app_key,\n",
    "            app_secret=app_secret,\n",
    "            oauth2_refresh_token=refresh_token\n",
    "        )\n",
    "\n",
    "        # Check that the access token is valid\n",
    "        account = dbx.users_get_current_account()\n",
    "\n",
    "        print(f\"\u2705 Connected to Dropbox account: {account.name.display_name}\")\n",
    "\n",
    "        return True, dbx\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"\u274c Error connecting to Dropbox: {e}\")\n",
    "\n",
    "        return False, None\n",
    "\n",
    "# Validate Dropbox credentials\n",
    "credentials_valid, dbx = validate_dropbox_credentials(APP_KEY, APP_SECRET, REFRESH_TOKEN)\n",
    "\n",
    "if not credentials_valid:\n",
    "\n",
    "    print(\"Please check your Dropbox credentials and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dropbox(file_path, dropbox_path, max_retries=3):\n",
    "\n",
    "    \"\"\"Upload a file to Dropbox with retry logic.\"\"\"\n",
    "    if not credentials_valid:\n",
    "\n",
    "        print(\"Dropbox credentials are not valid. Cannot upload.\")\n",
    "\n",
    "        return False\n",
    "        \n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(file_path, 'rb') as f:\n",
    "\n",
    "                # For small files, upload in one go\n",
    "                if file_size <= chunk_size:\n",
    "\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path}...\")\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        dbx.files_upload(f.read(), dropbox_path, mode=WriteMode('overwrite'))\n",
    "\n",
    "                        print(\"Upload complete!\")\n",
    "\n",
    "                        return True\n",
    "                    except ApiError as e:\n",
    "\n",
    "                        print(f\"ERROR: Dropbox API error - {e}\")\n",
    "\n",
    "                        if attempt < max_retries - 1:\n",
    "\n",
    "                            print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "\n",
    "                            continue\n",
    "                        return False\n",
    "                \n",
    "                # For large files, use chunked upload\n",
    "                else:\n",
    "\n",
    "                    print(f\"Uploading {file_path} to Dropbox as {dropbox_path} in chunks...\")\n",
    "\n",
    "                    upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "\n",
    "                    cursor = dropbox.files.UploadSessionCursor(\n",
    "                        session_id=upload_session_start_result.session_id,\n",
    "                        offset=f.tell()\n",
    "\n",
    "                    )\n",
    "\n",
    "                    commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode('overwrite'))\n",
    "\n",
    "                    # Upload the file in chunks with progress tracking\n",
    "                    uploaded = f.tell()\n",
    "\n",
    "                    with tqdm(total=file_size, desc=\"Uploading\", unit=\"B\", unit_scale=True) as pbar:\n",
    "\n",
    "                        pbar.update(uploaded)\n",
    "\n",
    "                        while uploaded < file_size:\n",
    "\n",
    "                            if (file_size - uploaded) <= chunk_size:\n",
    "\n",
    "                                dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "\n",
    "                                uploaded = file_size\n",
    "                                pbar.update(file_size - pbar.n)\n",
    "\n",
    "                            else:\n",
    "\n",
    "                                dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "\n",
    "                                uploaded = f.tell()\n",
    "\n",
    "                                cursor.offset = uploaded\n",
    "                                pbar.update(chunk_size)\n",
    "\n",
    "                    print(\"Chunked upload complete!\")\n",
    "\n",
    "                    return True\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"ERROR: Upload failed - {e}\")\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "\n",
    "                print(f\"Retrying... (Attempt {attempt+1}/{max_retries})\")\n",
    "\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "\n",
    "                print(\"Maximum retries reached. Upload failed.\")\n",
    "\n",
    "                return False\n",
    "    return False\n",
    "\n",
    "def create_shared_link(dropbox_path):\n",
    "\n",
    "    \"\"\"Create a shared link for a file in Dropbox.\"\"\"\n",
    "    if not credentials_valid:\n",
    "\n",
    "        print(\"Dropbox credentials are not valid. Cannot create shared link.\")\n",
    "\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "\n",
    "        shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
    "\n",
    "        return shared_link.url\n",
    "    except ApiError as e:\n",
    "\n",
    "        # If the file already has a shared link, the API will return an error\n",
    "        if isinstance(e.error, dropbox.sharing.CreateSharedLinkWithSettingsError) and \\\n",
    "           e.error.is_path() and e.error.get_path().is_shared_link_already_exists():\n",
    "\n",
    "            # Get existing shared links\n",
    "            shared_links = dbx.sharing_list_shared_links(path=dropbox_path).links\n",
    "            if shared_links:\n",
    "\n",
    "                return shared_links[0].url\n",
    "        print(f\"ERROR: Could not create shared link - {e}\")\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model zip to Dropbox\n",
    "if credentials_valid:\n",
    "\n",
    "    zip_path = f\"{model_save_dir}.zip\"\n",
    "    dropbox_path = f\"/codebert-swift-model/{os.path.basename(zip_path)}\"\n",
    "    \n",
    "    if upload_to_dropbox(zip_path, dropbox_path):\n",
    "\n",
    "        print(f\"Successfully uploaded model to Dropbox at {dropbox_path}\")\n",
    "\n",
    "        shared_link = create_shared_link(dropbox_path)\n",
    "\n",
    "        if shared_link:\n",
    "\n",
    "            print(f\"Shared link: {shared_link}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Failed to upload model to Dropbox.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Skipping Dropbox upload due to invalid credentials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Set up our environment with proper accelerator detection (CPU, GPU, or TPU)\n",
    "2. Loaded and preprocessed the Swift code dataset with error handling and validation\n",
    "3. Fine-tuned the CodeBERT model for Swift code classification with optimized training parameters\n",
    "4. Evaluated the model's performance with comprehensive metrics\n",
    "5. Saved and uploaded the model to Dropbox for easy access\n",
    "\n",
    "The model can now be used for Swift code understanding tasks, such as identifying Package.swift files. This is just one example of how CodeBERT can be fine-tuned for code-related tasks. The same approach can be extended to other programming languages and tasks, such as code search, code completion, and bug detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}