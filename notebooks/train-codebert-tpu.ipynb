{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding (TPU Version)\n",
    "\n",
    "In this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint) using TPU acceleration. CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n",
    "\n",
    "Unlike the previous version that focused only on identifying Package.swift files, this enhanced version trains the model on the entire dataset by classifying Swift files into meaningful categories based on their purpose in a codebase.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The process of fine-tuning CodeBERT involves:\n",
    "\n",
    "1. **ðŸ”§ Setup**: Install necessary libraries and prepare our TPU environment\n",
    "2. **ðŸ“¥ Data Loading**: Load the Swift code dataset from Hugging Face with detailed logging\n",
    "3. **ðŸ§¹ Enhanced Preprocessing**: Prepare the data for training by categorizing files and tokenizing the code samples\n",
    "4. **ðŸ§  TPU-Accelerated Model Training**: Fine-tune CodeBERT on our prepared data using TPU acceleration\n",
    "5. **ðŸ“Š Evaluation**: Assess how well our model performs with comprehensive metrics\n",
    "6. **ðŸ“¤ Export & Upload**: Save the model and upload it to Dropbox\n",
    "\n",
    "Let's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for TPU training\n",
    "!pip install -q cloud-tpu-client==0.10 torch==1.13.1 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp38-cp38-linux_x86_64.whl\n",
    "!pip install -q transformers datasets evaluate scikit-learn tqdm dropbox requests pandas matplotlib\n",
    "\n",
    "# Log installation process\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('codebert_tpu_training.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('codebert-tpu')\n",
    "\n",
    "logger.info(\"Starting CodeBERT TPU training notebook setup\")\n",
    "logger.info(f\"Notebook execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "logger.info(\"Required libraries installation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# Import AdamW from torch.optim\n",
    "from torch.optim import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Add memory management function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear XLA cache if available.\"\"\"\n",
    "    gc.collect()\n",
    "    if 'torch_xla' in sys.modules:\n",
    "        xm.rendezvous('cleanup_memory')\n",
    "        xm.mark_step()\n",
    "    logger.info(\"Memory cleaned up.\")\n",
    "\n",
    "logger.info(\"Libraries imported successfully\")\n",
    "logger.info(f\"Random seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## TPU Detection and Configuration\n",
    "\n",
    "Let's detect and configure the TPU environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU detection and configuration\n",
    "def detect_and_configure_tpu():\n",
    "    \"\"\"Detect and configure TPU for training.\"\"\"\n",
    "    try:\n",
    "        # Check if TPU is available\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        logger.info(\"TPU libraries imported successfully\")\n",
    "        \n",
    "        # Get TPU device\n",
    "        device = xm.xla_device()\n",
    "        logger.info(f\"TPU device detected: {device}\")\n",
    "        \n",
    "        # Get TPU core count\n",
    "        tpu_cores = xm.xrt_world_size()\n",
    "        logger.info(f\"Number of TPU cores available: {tpu_cores}\")\n",
    "        \n",
    "        return device, tpu_cores\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error detecting TPU: {e}\")\n",
    "        logger.warning(\"Falling back to CPU. This notebook is designed for TPU and may not work optimally on CPU.\")\n",
    "        return torch.device('cpu'), 1\n",
    "\n",
    "# Detect and configure TPU\n",
    "device, tpu_cores = detect_and_configure_tpu()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "logger.info(f\"Using device: {device}\")\n",
    "logger.info(f\"Random seeds set for reproducibility: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using with TPU-optimized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration - optimized for TPU\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# TPU-optimized batch sizes and training parameters\n",
    "# Adjust batch size based on TPU cores available\n",
    "BATCH_SIZE = 8 * tpu_cores  # Scale batch size with TPU cores\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 500\n",
    "GRADIENT_ACCUMULATION_STEPS = 1  # Reduced for TPU efficiency\n",
    "\n",
    "logger.info(\"Configuration parameters:\")\n",
    "logger.info(f\"Dataset: {DATASET_ID}\")\n",
    "logger.info(f\"Model: {MODEL_NAME}\")\n",
    "logger.info(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "logger.info(f\"Batch size: {BATCH_SIZE} (scaled for {tpu_cores} TPU cores)\")\n",
    "logger.info(f\"Learning rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "logger.info(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"Warmup steps: {WARMUP_STEPS}\")\n",
    "logger.info(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling and detailed logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic and detailed logging\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic and detailed logging.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            start_time = time.time()\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            load_time = time.time() - start_time\n",
    "            logger.info(f\"Dataset loaded successfully in {load_time:.2f} seconds\")\n",
    "            logger.info(f\"Dataset contains {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                logger.error(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Make sure dataset ID is defined (in case previous cell didn't execute)\n",
    "if 'DATASET_ID' not in globals():\n",
    "    logger.warning(\"DATASET_ID not found. Using default value.\")\n",
    "    DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"  # Default value as fallback\n",
    "    MAX_LENGTH = 384\n",
    "    MODEL_NAME = \"microsoft/codebert-base\"\n",
    "    BATCH_SIZE = 8 * tpu_cores\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    logger.info(\"Using default configuration values.\")\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    logger.info(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    logger.info(\"Dataset structure:\")\n",
    "    logger.info(str(data))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names with detailed logging\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    \n",
    "    logger.info(\"Verifying dataset structure...\")\n",
    "    \n",
    "    if 'train' not in dataset:\n",
    "        logger.warning(\"Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    # Log available columns\n",
    "    available_columns = dataset['train'].column_names\n",
    "    logger.info(f\"Available columns: {available_columns}\")\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in available_columns]\n",
    "    if missing_columns:\n",
    "        logger.warning(f\"Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    logger.info(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    logger.warning(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset with detailed logging\n",
    "try:\n",
    "    logger.info(\"Exploring dataset example...\")\n",
    "    \n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "        split_name = 'train'\n",
    "    else:\n",
    "        split_name = list(data.keys())[0]\n",
    "        example = data[split_name][0]\n",
    "    \n",
    "    logger.info(f\"Example from '{split_name}' split:\")\n",
    "    \n",
    "    # Log each feature with appropriate truncation for long values\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str):\n",
    "            logger.info(f\"{key}: {value[:100]}...\" if len(value) > 100 else f\"{key}: {value}\")\n",
    "            # Additional logging for content analysis\n",
    "            if key == 'content':\n",
    "                logger.info(f\"Content length: {len(value)} characters\")\n",
    "                logger.info(f\"Content lines: {value.count('\\n')+1}\")\n",
    "        else:\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "    \n",
    "    # Log dataset statistics\n",
    "    logger.info(f\"Total examples in dataset: {sum(len(data[split]) for split in data)}\")\n",
    "    for split in data:\n",
    "        logger.info(f\"Examples in {split} split: {len(data[split])}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error exploring dataset example: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling and detailed logging\n",
    "try:\n",
    "    logger.info(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    logger.info(f\"Tokenizer loaded successfully in {load_time:.2f} seconds\")\n",
    "    logger.info(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    logger.info(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "    \n",
    "    # Log special tokens\n",
    "    logger.info(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "    logger.info(f\"Padding token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "    logger.info(f\"Unknown token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Enhanced Data Preparation\n",
    "\n",
    "Instead of focusing only on Package.swift files, we'll create a more meaningful multi-class classification task that categorizes Swift files based on their purpose in a codebase. This approach utilizes the entire dataset and provides more valuable insights into code understanding.\n",
    "\n",
    "We'll categorize files into the following classes:\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-data-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "def analyze_content_for_category(content):\n",
    "    \"\"\"\n",
    "    Analyze file content to help determine its category when path-based classification is ambiguous.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The file content\n",
    "        \n",
    "    Returns:\n",
    "        int: The suggested category based on content analysis\n",
    "    \"\"\"\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Check for model patterns\n",
    "    if (re.search(r'struct\\s+\\w+', content) or \n",
    "        re.search(r'class\\s+\\w+\\s*:\\s*\\w*codable', content_lower) or\n",
    "        'encodable' in content_lower or 'decodable' in content_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Check for view patterns\n",
    "    elif ('uiview' in content_lower or \n",
    "          'uitableview' in content_lower or \n",
    "          'uicollectionview' in content_lower or\n",
    "          'swiftui' in content_lower or\n",
    "          'view {' in content_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Check for controller patterns\n",
    "    elif ('viewcontroller' in content_lower or \n",
    "          'uiviewcontroller' in content_lower or\n",
    "          'navigationcontroller' in content_lower or\n",
    "          'viewdidload' in content_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Check for utility patterns\n",
    "    elif ('extension' in content_lower or \n",
    "          'func ' in content and not 'class' in content_lower[:100] or\n",
    "          'protocol' in content_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Check for test patterns\n",
    "    elif ('xctest' in content_lower or \n",
    "          'testcase' in content_lower or\n",
    "          'func test' in content_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Check for configuration patterns\n",
    "    elif ('package(' in content_lower or \n",
    "          'dependencies' in content_lower and 'package' in content_lower or\n",
    "          'products' in content_lower and 'targets' in content_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "# Process the dataset to add category labels with detailed logging\n",
    "def process_dataset_with_categories(dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset to add category labels based on file path and content analysis.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The Hugging Face dataset\n",
    "        \n",
    "    Returns:\n",
    "        processed_dataset: The dataset with added category labels\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing dataset to add category labels...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define category names for better interpretability\n",
    "    category_names = [\n",
    "        \"Models\",        # 0\n",
    "        \"Views\",         # 1\n",
    "        \"Controllers\",   # 2\n",
    "        \"Utilities\",     # 3\n",
    "        \"Tests\",         # 4\n",
    "        \"Configuration\"  # 5\n",
    "    ]\n",
    "    \n",
    "    # Function to determine category from path and content\n",
    "    def determine_category(example):\n",
    "        path_category = extract_file_type(example['path'])\n",
    "        content_category = analyze_content_for_category(example['content'])\n",
    "        \n",
    "        # If path and content analysis agree, use that category\n",
    "        # Otherwise, prioritize path-based categorization\n",
    "        final_category = path_category if path_category == content_category else path_category\n",
    "        \n",
    "        return {\n",
    "            'category': final_category,\n",
    "            'category_name': category_names[final_category]\n",
    "        }\n",
    "    \n",
    "    # Process each split in the dataset\n",
    "    processed_dataset = {}\n",
    "    category_counts = {name: 0 for name in category_names}\n",
    "    \n",
    "    for split in dataset:\n",
    "        logger.info(f\"Processing {split} split...\")\n",
    "        \n",
    "        # Map the determine_category function to each example\n",
    "        processed_split = dataset[split].map(\n",
    "            determine_category,\n",
    "            desc=f\"Adding categories to {split} split\"\n",
    "        )\n",
    "        \n",
    "        # Count categories for logging\n",
    "        split_categories = processed_split['category']\n",
    "        for i, name in enumerate(category_names):\n",
    "            count = split_categories.count(i)\n",
    "            category_counts[name] += count\n",
    "            logger.info(f\"  {name}: {count} files\")\n",
    "        \n",
    "        processed_dataset[split] = processed_split\n",
    "    \n",
    "    # Log overall category distribution\n",
    "    logger.info(\"Overall category distribution:\")\n",
    "    for name, count in category_counts.items():\n",
    "        logger.info(f\"  {name}: {count} files ({count/sum(category_counts.values())*100:.2f}%)\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    logger.info(f\"Dataset processing completed in {processing_time:.2f} seconds\")\n",
    "    \n",
    "    return processed_dataset, category_names\n",
    "\n",
    "# Process the dataset to add category labels\n",
    "try:\n",
    "    processed_data, category_names = process_dataset_with_categories(data)\n",
    "    num_labels = len(category_names)\n",
    "    logger.info(f\"Processed dataset with {num_labels} categories\")\n",
    "    \n",
    "    # Create class weights for handling imbalanced classes\n",
    "    if 'train' in processed_data:\n",
    "        category_counts = collections.Counter(processed_data['train']['category'])\n",
    "        total_samples = len(processed_data['train'])\n",
    "        \n",
    "        # Calculate class weights inversely proportional to class frequencies\n",
    "        class_weights = torch.tensor(\n",
    "            [total_samples / (len(category_counts) * count) for label, count in sorted(category_counts.items())],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Class weights calculated: {class_weights}\")\n",
    "        \n",
    "        # Move class weights to the appropriate device\n",
    "        class_weights = class_weights.to(device)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-split-section",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Now let's split our data into training and validation sets with stratification to maintain label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(dataset, val_size=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Split a dataset into training and validation sets with stratification.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to split\n",
    "        val_size: The proportion of the dataset to include in the validation split\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_dataset, val_dataset: The split datasets\n",
    "    \"\"\"\n",
    "    logger.info(f\"Splitting dataset with validation size {val_size} and seed {seed}...\")\n",
    "    \n",
    "    # If dataset already has train/validation splits, use those\n",
    "    if 'train' in dataset and 'validation' in dataset:\n",
    "        logger.info(\"Dataset already has train/validation splits. Using existing splits.\")\n",
    "        return dataset['train'], dataset['validation']\n",
    "    \n",
    "    # If dataset only has train split, create validation split\n",
    "    if 'train' in dataset and 'validation' not in dataset:\n",
    "        train_data = dataset['train']\n",
    "    else:\n",
    "        # Use the first available split\n",
    "        split_name = list(dataset.keys())[0]\n",
    "        train_data = dataset[split_name]\n",
    "        logger.info(f\"Using '{split_name}' split for training data\")\n",
    "    \n",
    "    # Convert to pandas for easier splitting\n",
    "    df = train_data.to_pandas()\n",
    "    \n",
    "    # Split with stratification by category\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=val_size, \n",
    "        random_state=seed,\n",
    "        stratify=df['category'] if 'category' in df.columns else None\n",
    "    )\n",
    "    \n",
    "    # Log split sizes and category distributions\n",
    "    logger.info(f\"Training set size: {len(train_df)}\")\n",
    "    logger.info(f\"Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    if 'category' in df.columns:\n",
    "        logger.info(\"Category distribution in training set:\")\n",
    "        train_cat_dist = train_df['category'].value_counts(normalize=True)\n",
    "        for cat, prop in train_cat_dist.items():\n",
    "            logger.info(f\"  Category {cat} ({category_names[cat]}): {prop*100:.2f}%\")\n",
    "            \n",
    "        logger.info(\"Category distribution in validation set:\")\n",
    "        val_cat_dist = val_df['category'].value_counts(normalize=True)\n",
    "        for cat, prop in val_cat_dist.items():\n",
    "            logger.info(f\"  Category {cat} ({category_names[cat]}): {prop*100:.2f}%\")\n",
    "    \n",
    "    # Convert back to Hugging Face datasets\n",
    "    from datasets import Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    logger.info(\"Dataset splitting completed successfully\")\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Split the dataset\n",
    "try:\n",
    "    train_data, val_data = split_dataset(processed_data, val_size=0.1, seed=SEED)\n",
    "    logger.info(f\"Split dataset into {len(train_data)} training and {len(val_data)} validation examples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error splitting dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-section",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now let's tokenize our data for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the code content with proper truncation and padding.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples to tokenize\n",
    "        \n",
    "    Returns:\n",
    "        tokenized_examples: The tokenized examples\n",
    "    \"\"\"\n",
    "    # Tokenize the code content\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['content'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    tokenized_examples[\"labels\"] = examples[\"category\"]\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "# Tokenize the datasets with detailed logging\n",
    "try:\n",
    "    logger.info(\"Tokenizing datasets...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tokenize training data\n",
    "    logger.info(\"Tokenizing training data...\")\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing training data\",\n",
    "        remove_columns=train_data.column_names\n",
    "    )\n",
    "    \n",
    "    # Tokenize validation data\n",
    "    logger.info(\"Tokenizing validation data...\")\n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing validation data\",\n",
    "        remove_columns=val_data.column_names\n",
    "    )\n",
    "    \n",
    "    tokenization_time = time.time() - start_time\n",
    "    logger.info(f\"Tokenization completed in {tokenization_time:.2f} seconds\")\n",
    "    logger.info(f\"Tokenized {len(tokenized_train_data)} training examples\")\n",
    "    logger.info(f\"Tokenized {len(tokenized_val_data)} validation examples\")\n",
    "    \n",
    "    # Log tokenized data structure\n",
    "    logger.info(f\"Tokenized data features: {tokenized_train_data.column_names}\")\n",
    "    \n",
    "    # Log token statistics for a sample\n",
    "    sample_lengths = [len(x) for x in tokenized_train_data['input_ids'][:100]]\n",
    "    logger.info(f\"Average sequence length in sample: {sum(sample_lengths)/len(sample_lengths):.1f} tokens\")\n",
    "    logger.info(f\"Max sequence length in sample: {max(sample_lengths)} tokens\")\n",
    "    logger.info(f\"Min sequence length in sample: {min(sample_lengths)} tokens\")\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train_data.set_format(\"torch\")\n",
    "    tokenized_val_data.set_format(\"torch\")\n",
    "    logger.info(\"Dataset format set to PyTorch tensors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Now let's load the CodeBERT model and prepare it for TPU training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the model with the correct number of labels\n",
    "    logger.info(f\"Loading model {MODEL_NAME} with {num_labels} output classes...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Log model architecture\n",
    "    logger.info(f\"Model type: {model.__class__.__name__}\")\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Move model to TPU device\n",
    "    model.to(device)\n",
    "    logger.info(f\"Model moved to device: {device}\")\n",
    "    \n",
    "    model_load_time = time.time() - start_time\n",
    "    logger.info(f\"Model loaded in {model_load_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class-weights-section",
   "metadata": {},
   "source": [
    "## Class Weights for Imbalanced Data\n",
    "\n",
    "Let's calculate class weights to handle any imbalance in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "try:\n",
    "    logger.info(\"Visualizing class distribution...\")\n",
    "    \n",
    "    # Count labels in training data\n",
    "    label_counts = collections.Counter(tokenized_train_data['labels'].numpy())\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    label_df = pd.DataFrame({\n",
    "        'Category': [category_names[i] for i in range(num_labels)],\n",
    "        'Count': [label_counts.get(i, 0) for i in range(num_labels)]\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = label_df['Count'].sum()\n",
    "    label_df['Percentage'] = label_df['Count'] / total * 100\n",
    "    \n",
    "    # Log class distribution\n",
    "    logger.info(\"Class distribution in training data:\")\n",
    "    for _, row in label_df.iterrows():\n",
    "        logger.info(f\"  {row['Category']}: {row['Count']} examples ({row['Percentage']:.2f}%)\")\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(label_df['Category'], label_df['Count'])\n",
    "    plt.title('Class Distribution in Training Data')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Number of Examples')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{height:,}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png')\n",
    "    logger.info(\"Class distribution plot saved to 'class_distribution.png'\")\n",
    "    \n",
    "    # Calculate class weights if not already done\n",
    "    if 'class_weights' not in globals():\n",
    "        # Calculate class weights inversely proportional to class frequencies\n",
    "        class_weights = torch.tensor(\n",
    "            [total / (num_labels * count) for label, count in sorted(label_counts.items())],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        # Move class weights to the appropriate device\n",
    "        class_weights = class_weights.to(device)\n",
    "        \n",
    "    logger.info(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error visualizing class distribution: {e}\")\n",
    "    # If visualization fails, ensure we still have class weights\n",
    "    if 'class_weights' not in globals():\n",
    "        logger.warning(\"Using equal class weights due to error\")\n",
    "        class_weights = torch.ones(num_labels, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## TPU-Optimized Training Setup\n",
    "\n",
    "Let's set up the training configuration optimized for TPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom loss function with class weights for TPU\n",
    "class TPUWeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"Custom loss function with class weights for TPU.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Use class weights in the loss calculation\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        # Log batch loss\n",
    "        if self.state.global_step % 10 == 0:\n",
    "            logger.info(f\"Step {self.state.global_step}: Batch loss = {loss.item():.4f}\")\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"Custom training step with TPU synchronization.\"\"\"\n",
    "        # Regular training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "        \n",
    "        # TPU-specific: Mark step for XLA compilation\n",
    "        if 'torch_xla' in sys.modules:\n",
    "            xm.mark_step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Define TPU-optimized training arguments\n",
    "logger.info(\"Setting up TPU-optimized training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_tpu\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs_tpu\",\n",
    "    logging_steps=10,  # More frequent logging\n",
    "    save_total_limit=2,\n",
    "    # TPU-specific settings\n",
    "    dataloader_drop_last=True,  # Important for TPU to have fixed size batches\n",
    "    dataloader_num_workers=4,   # Parallel data loading\n",
    "    fp16=False,                 # TPU uses bfloat16 instead of fp16\n",
    "    bf16=True,                  # Use bfloat16 precision for TPU\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Log training arguments\n",
    "logger.info(\"Training arguments:\")\n",
    "for key, value in training_args.to_dict().items():\n",
    "    logger.info(f\"  {key}: {value}\")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "logger.info(\"Early stopping callback configured with patience=3, threshold=0.01\")\n",
    "\n",
    "# Define compute_metrics function for evaluation with detailed logging\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics with detailed logging.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision, per_class_recall, per_class_f1, per_class_support = \\\n",
    "        precision_recall_fscore_support(labels, predictions, average=None)\n",
    "    \n",
    "    # Log detailed metrics\n",
    "    logger.info(\"Evaluation metrics:\")\n",
    "    logger.info(f\"  Accuracy: {acc:.4f}\")\n",
    "    logger.info(f\"  Weighted F1: {f1:.4f}\")\n",
    "    logger.info(f\"  Weighted Precision: {precision:.4f}\")\n",
    "    logger.info(f\"  Weighted Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Log per-class metrics\n",
    "    logger.info(\"Per-class metrics:\")\n",
    "    for i in range(num_labels):\n",
    "        logger.info(f\"  Class {i} ({category_names[i]}):\\n\"\n",
    "                   f\"    Precision: {per_class_precision[i]:.4f}\\n\"\n",
    "                   f\"    Recall: {per_class_recall[i]:.4f}\\n\"\n",
    "                   f\"    F1: {per_class_f1[i]:.4f}\\n\"\n",
    "                   f\"    Support: {per_class_support[i]}\")\n",
    "    \n",
    "    # Calculate and log confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    logger.info(f\"Confusion matrix:\\n{cm}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Create data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "logger.info(\"Data collator created for padding\")\n",
    "\n",
    "# Create trainer with weighted loss for TPU\n",
    "logger.info(\"Creating TPU-optimized trainer...\")\n",
    "trainer = TPUWeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "logger.info(\"TPU-optimized training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## TPU-Accelerated Model Training\n",
    "\n",
    "Now let's train the model using TPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Starting TPU-accelerated training...\")\n",
    "    logger.info(f\"Training on device: {device}\")\n",
    "    logger.info(f\"Number of TPU cores: {tpu_cores}\")\n",
    "    logger.info(f\"Training for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE}\")\n",
    "    \n",
    "    # Record start time for training\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - training_start_time\n",
    "    \n",
    "    # Log detailed training results\n",
    "    logger.info(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    logger.info(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    logger.info(f\"Training steps: {train_result.metrics['train_runtime']/train_result.metrics['train_steps_per_second']:.0f}\")\n",
    "    logger.info(f\"Steps per second: {train_result.metrics['train_steps_per_second']:.2f}\")\n",
    "    \n",
    "    # Log all metrics\n",
    "    logger.info(\"All training metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        logger.info(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save the model\n",
    "    logger.info(\"Saving the final model...\")\n",
    "    trainer.save_model(\"./final_model_tpu\")\n",
    "    logger.info(\"Model saved to ./final_model_tpu\")\n",
    "    \n",
    "    # Save tokenizer alongside the model\n",
    "    tokenizer.save_pretrained(\"./final_model_tpu\")\n",
    "    logger.info(\"Tokenizer saved with the model\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    logger.info(\"Memory cleaned up after training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation\n",
    "\n",
    "Let's evaluate our model on the validation set with detailed metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Starting comprehensive model evaluation...\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    logger.info(\"Evaluating model on validation set...\")\n",
    "    eval_start_time = time.time()\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_time = time.time() - eval_start_time\n",
    "    \n",
    "    # Log evaluation results\n",
    "    logger.info(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
    "    logger.info(\"Evaluation results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        logger.info(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Get predictions for detailed analysis\n",
    "    logger.info(\"Generating predictions for detailed analysis...\")\n",
    "    predictions = trainer.predict(tokenized_val_data)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(category_names))\n",
    "    plt.xticks(tick_marks, category_names, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, category_names)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    logger.info(\"Confusion matrix saved to 'confusion_matrix.png'\")\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    \n",
    "    # Create a DataFrame for metrics visualization\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Category': category_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df.to_csv('class_metrics.csv', index=False)\n",
    "    logger.info(\"Per-class metrics saved to 'class_metrics.csv'\")\n",
    "    \n",
    "    # Plot F1 scores by class\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metrics_df['Category'], metrics_df['F1 Score'])\n",
    "    plt.title('F1 Score by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('f1_scores.png')\n",
    "    logger.info(\"F1 scores plot saved to 'f1_scores.png'\")\n",
    "    \n",
    "    # Log detailed per-class metrics\n",
    "    logger.info(\"Detailed per-class metrics:\")\n",
    "    for i, category in enumerate(category_names):\n",
    "        logger.info(f\"  {category}:\\n\"\n",
    "                   f\"    Precision: {precision[i]:.4f}\\n\"\n",
    "                   f\"    Recall: {recall[i]:.4f}\\n\"\n",
    "                   f\"    F1 Score: {f1[i]:.4f}\\n\"\n",
    "                   f\"    Support: {support[i]}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    overall_accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    logger.info(\"Overall metrics:\")\n",
    "    logger.info(f\"  Accuracy: {overall_accuracy:.4f}\")\n",
    "    logger.info(f\"  Weighted Precision: {overall_precision:.4f}\")\n",
    "    logger.info(f\"  Weighted Recall: {overall_recall:.4f}\")\n",
    "    logger.info(f\"  Weighted F1 Score: {overall_f1:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-export-section",
   "metadata": {},
   "source": [
    "## Model Export and Upload\n",
    "\n",
    "Let's export our model and upload it to Dropbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "def zip_model_directory(model_dir, output_zip):\n",
    "    \"\"\"Zip the model directory for easier upload.\"\"\"\n",
    "    logger.info(f\"Zipping model directory {model_dir} to {output_zip}...\")\n",
    "    \n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(model_dir))\n",
    "                logger.info(f\"  Adding {arcname} to zip\")\n",
    "                zipf.write(file_path, arcname)\n",
    "    \n",
    "    zip_size = os.path.getsize(output_zip) / (1024 * 1024)  # Size in MB\n",
    "    logger.info(f\"Model zipped successfully. Zip size: {zip_size:.2f} MB\")\n",
    "    return output_zip\n",
    "\n",
    "def upload_to_dropbox(file_path, dropbox_token, dropbox_path):\n",
    "    \"\"\"Upload a file to Dropbox.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Uploading {file_path} to Dropbox path {dropbox_path}...\")\n",
    "        \n",
    "        # Initialize Dropbox client\n",
    "        dbx = dropbox.Dropbox(dropbox_token)\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        logger.info(f\"File size: {file_size / (1024 * 1024):.2f} MB\")\n",
    "        \n",
    "        # Upload file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            if file_size <= 150 * 1024 * 1024:  # 150 MB limit for simple uploads\n",
    "                logger.info(\"Using simple upload\")\n",
    "                dbx.files_upload(f.read(), dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "            else:\n",
    "                logger.info(\"Using chunked upload for large file\")\n",
    "                chunk_size = 4 * 1024 * 1024  # 4 MB chunks\n",
    "                upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "                cursor = dropbox.files.UploadSessionCursor(\n",
    "                    session_id=upload_session_start_result.session_id,\n",
    "                    offset=f.tell()\n",
    "                )\n",
    "                commit = dropbox.files.CommitInfo(path=dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "                \n",
    "                while f.tell() < file_size:\n",
    "                    if (file_size - f.tell()) <= chunk_size:\n",
    "                        dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "                        break\n",
    "                    else:\n",
    "                        dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "                        cursor.offset = f.tell()\n",
    "                        logger.info(f\"Uploaded {cursor.offset / (1024 * 1024):.2f} MB so far\")\n",
    "        \n",
    "        # Create a shared link\n",
    "        shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
    "        download_url = shared_link.url.replace('www.dropbox.com', 'dl.dropboxusercontent.com')\n",
    "        \n",
    "        logger.info(f\"Upload successful!\")\n",
    "        logger.info(f\"Shared link: {shared_link.url}\")\n",
    "        logger.info(f\"Direct download link: {download_url}\")\n",
    "        \n",
    "        return download_url\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to Dropbox: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "# Save model metadata\n",
    "try:\n",
    "    logger.info(\"Saving model metadata...\")\n",
    "    \n",
    "    # Create metadata dictionary\n",
    "    metadata = {\n",
    "        \"model_name\": \"CodeBERT-Swift-TPU\",\n",
    "        \"base_model\": MODEL_NAME,\n",
    "        \"num_labels\": num_labels,\n",
    "        \"categories\": category_names,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"training_params\": {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"warmup_steps\": WARMUP_STEPS,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"tpu_cores\": tpu_cores\n",
    "        },\n",
    "        \"dataset\": DATASET_ID,\n",
    "        \"training_examples\": len(tokenized_train_data),\n",
    "        \"validation_examples\": len(tokenized_val_data),\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(overall_accuracy),\n",
    "            \"f1\": float(overall_f1),\n",
    "            \"precision\": float(overall_precision),\n",
    "            \"recall\": float(overall_recall)\n",
    "        },\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save metadata to file\n",
    "    with open(\"./final_model_tpu/metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    logger.info(\"Model metadata saved successfully\")\n",
    "    \n",
    "    # Zip the model directory\n",
    "    model_zip = zip_model_directory(\"./final_model_tpu\", \"codebert_swift_tpu.zip\")\n",
    "    \n",
    "    # Upload to Dropbox if token is provided\n",
    "    # Note: In a real notebook, you would prompt for the token or use environment variables\n",
    "    dropbox_token = input(\"Enter your Dropbox access token (leave empty to skip upload): \").strip()\n",
    "    \n",
    "    if dropbox_token:\n",
    "        dropbox_path = \"/codebert_swift_tpu.zip\"\n",
    "        download_url = upload_to_dropbox(model_zip, dropbox_token, dropbox_path)\n",
    "        \n",
    "        if download_url:\n",
    "            # Save download URL to metadata\n",
    "            metadata[\"download_url\"] = download_url\n",
    "            with open(\"./final_model_tpu/metadata.json\", \"w\") as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            logger.info(\"Metadata updated with download URL\")\n",
    "    else:\n",
    "        logger.info(\"Dropbox upload skipped\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during model export: {e}\")\n",
    "    import traceback\n",
    "    logger.error(f\"Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully enhanced the CodeBERT training process to utilize TPU acceleration while maintaining the same functionality as the original notebook. Our model now classifies Swift code files into meaningful categories based on their purpose in a codebase:\n",
    "\n",
    "1. **Models** - Data structures and model definitions\n",
    "2. **Views** - UI related files\n",
    "3. **Controllers** - Application logic\n",
    "4. **Utilities** - Helper functions and extensions\n",
    "5. **Tests** - Test files\n",
    "6. **Configuration** - Package and configuration files\n",
    "\n",
    "The TPU-accelerated training provides significant performance improvements over CPU or even GPU training, allowing us to train with larger batch sizes and potentially achieve better results in less time.\n",
    "\n",
    "### Key Improvements in this TPU Version:\n",
    "\n",
    "1. **TPU Optimization**: Configured the training pipeline to fully utilize TPU acceleration\n",
    "2. **Enhanced Logging**: Added comprehensive logging throughout the notebook for better tracking and debugging\n",
    "3. **Visualization**: Added plots and visualizations for better understanding of the data and results\n",
    "4. **Robust Error Handling**: Improved error handling and recovery mechanisms\n",
    "5. **Detailed Metrics**: Added more detailed evaluation metrics for better model assessment\n",
    "\n",
    "This notebook can be used as a template for other TPU-accelerated NLP tasks, especially those involving code understanding and classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}