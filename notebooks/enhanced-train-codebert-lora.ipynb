{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e1db4c-8bd6-47a1-8462-4e44407f70d4",
   "metadata": {},
   "source": [
    "# Enhanced CodeBERT for Swift Code Understanding with LoRA\n",
    "\n",
    "In this notebook, we fine-tune the [CodeBERT](https://github.com/microsoft/CodeBERT) model on the [Swift Code Intelligence dataset](https://huggingface.co/datasets/mvasiliniuc/iva-swift-codeint) using LoRA (Low-Rank Adaptation). CodeBERT is a pre-trained model specifically designed for programming languages, much like how BERT was pre-trained for natural language text. Created by Microsoft Research, CodeBERT can understand both programming language and natural language, making it ideal for code-related tasks.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a technique that significantly reduces the number of trainable parameters by adding small, trainable rank decomposition matrices to the existing weights rather than fine-tuning all model parameters. This approach:\n",
    "\n",
    "- Reduces memory usage by up to 3-4x\n",
    "- Speeds up training significantly\n",
    "- Allows for efficient model adaptation with minimal parameters\n",
    "- Maintains model quality comparable to full fine-tuning\n",
    "\n",
    "## Overview\n",
    "\n",
    "The process of fine-tuning CodeBERT with LoRA involves:\n",
    "\n",
    "1. **ðŸ”§ Setup**: Install necessary libraries and prepare our environment\n",
    "2. **ðŸ“¥ Data Loading**: Load the Swift code dataset from Hugging Face\n",
    "3. **ðŸ§¹ Preprocessing**: Prepare the data for training by tokenizing the code samples\n",
    "4. **ðŸ”„ LoRA Configuration**: Set up LoRA for efficient fine-tuning\n",
    "5. **ðŸ§  Model Training**: Fine-tune CodeBERT on our prepared data with optimized performance\n",
    "6. **ðŸ“Š Evaluation**: Assess how well our model performs\n",
    "7. **ðŸ“¤ Export & Upload**: Save the model and upload it to Dropbox\n",
    "\n",
    "Let's start by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    default_data_collator,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# For memory optimization\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Import PEFT for LoRA\n",
    "from peft import (\n",
    "    get_peft_model, \n",
    "    LoraConfig, \n",
    "    TaskType, \n",
    "    PeftModel, \n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accelerator-detection",
   "metadata": {},
   "source": [
    "## Accelerator Detection and Configuration\n",
    "\n",
    "Let's detect and configure the available accelerator (CPU, GPU, or TPU) with enhanced detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-accelerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and configure accelerator with better error handling\n",
    "def detect_and_configure_accelerator():\n",
    "    \"\"\"Detect and configure the available accelerator (CPU, GPU, or TPU) with enhanced detection.\"\"\"\n",
    "    try:\n",
    "        # Initialize accelerator from HF accelerate library\n",
    "        accelerator = Accelerator()\n",
    "        if accelerator.distributed_type == \"TPU\":\n",
    "            print(\"TPU detected! Configuring for TPU training...\")\n",
    "            device = accelerator.device\n",
    "            use_tpu = True\n",
    "            use_gpu = False\n",
    "            use_mixed_precision = True\n",
    "            return device, use_tpu, use_gpu, use_mixed_precision, accelerator\n",
    "        \n",
    "        # Check for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU detected! Using {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            use_tpu = False\n",
    "            use_gpu = True\n",
    "            use_mixed_precision = True  # Enable mixed precision by default for GPU\n",
    "            print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "            \n",
    "            # Clear GPU cache to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(\"No GPU or TPU detected. Using CPU (this will be slow).\")\n",
    "            device = torch.device(\"cpu\")\n",
    "            use_tpu = False\n",
    "            use_gpu = False\n",
    "            use_mixed_precision = False  # Disable mixed precision for CPU\n",
    "        \n",
    "        return device, use_tpu, use_gpu, use_mixed_precision, accelerator\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting accelerator: {e}\")\n",
    "        print(\"Defaulting to CPU.\")\n",
    "        return torch.device(\"cpu\"), False, False, False, None\n",
    "\n",
    "# Detect and configure accelerator\n",
    "device, use_tpu, use_gpu, use_mixed_precision, accelerator = detect_and_configure_accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57034d-bc42-472f-abfd-04a797218141",
   "metadata": {},
   "source": [
    "## Dataset and Model Configuration\n",
    "\n",
    "Let's define the model and dataset we'll be using with optimized batch sizes and memory settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ab22-efd5-4270-9011-547028913250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and dataset IDs\n",
    "MODEL_ID = \"microsoft/codebert-base\"\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Configure batch sizes based on available hardware with optimized values\n",
    "# With LoRA, we can use larger batch sizes due to reduced memory requirements\n",
    "if use_tpu:\n",
    "    TRAIN_BATCH_SIZE = 128  # Larger batch size for TPU with LoRA\n",
    "    EVAL_BATCH_SIZE = 256\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    NUM_WORKERS = 8\n",
    "elif use_gpu:\n",
    "    # Dynamically adjust batch size based on available GPU memory\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory_gb > 16:  # High-end GPU\n",
    "        TRAIN_BATCH_SIZE = 64  # Increased due to LoRA efficiency\n",
    "        EVAL_BATCH_SIZE = 128\n",
    "        GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    elif gpu_memory_gb > 8:  # Mid-range GPU\n",
    "        TRAIN_BATCH_SIZE = 32  # Increased due to LoRA efficiency\n",
    "        EVAL_BATCH_SIZE = 64\n",
    "        GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    else:  # Low-end GPU\n",
    "        TRAIN_BATCH_SIZE = 16  # Increased due to LoRA efficiency\n",
    "        EVAL_BATCH_SIZE = 32\n",
    "        GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    NUM_WORKERS = min(4, os.cpu_count() or 1)\n",
    "else:\n",
    "    TRAIN_BATCH_SIZE = 8   # Increased for CPU with LoRA\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    NUM_WORKERS = 0  # No multiprocessing on CPU\n",
    "\n",
    "# Set maximum sequence length for tokenization\n",
    "MAX_SEQ_LENGTH = 512  # CodeBERT's maximum sequence length\n",
    "\n",
    "# Configure LoRA parameters\n",
    "LORA_R = 8        # Rank of the update matrices\n",
    "LORA_ALPHA = 16   # Scaling factor for the update matrices\n",
    "LORA_DROPOUT = 0.1  # Dropout probability for LoRA layers\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Training batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Evaluation batch size: {EVAL_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Number of dataloader workers: {NUM_WORKERS}\")\n",
    "print(f\"Using mixed precision: {use_mixed_precision}\")\n",
    "print(f\"LoRA configuration: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Now let's load the Swift code dataset and examine its structure with proper error handling and caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic and caching\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic and caching.\"\"\"\n",
    "    cache_dir = os.path.join(os.getcwd(), \"dataset_cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True, cache_dir=cache_dir)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    \n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at an example from the dataset\n",
    "try:\n",
    "    if 'train' in data:\n",
    "        example = data['train'][0]\n",
    "    else:\n",
    "        example = data[list(data.keys())[0]][0]\n",
    "        \n",
    "    print(\"Example features:\")\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset example: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## Loading the CodeBERT Tokenizer\n",
    "\n",
    "Now, let's load the CodeBERT tokenizer, which has been specially trained to handle code tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer with error handling and caching\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)  # Use fast tokenizer for better performance\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we're dealing with a code understanding task, we need to prepare our data appropriately. The dataset contains Swift code files, so we'll need to create labeled data for our task.\n",
    "\n",
    "For this demonstration, we'll create a binary classification task that determines whether the code is a Package.swift file (which is used for Swift package management) or not. This is just an example task - in a real application, you might have more complex classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset based on whether the file is a Package.swift file\n",
    "def add_labels(example):\n",
    "    # Label 1 if it's a Package.swift file, 0 otherwise\n",
    "    example['label'] = 1 if 'Package.swift' in example['path'] else 0\n",
    "    return example\n",
    "\n",
    "try:\n",
    "    # Apply the labeling function\n",
    "    labeled_data = data['train'].map(add_labels)\n",
    "    \n",
    "    # Check the distribution of labels using collections.Counter\n",
    "    import collections\n",
    "    all_labels = labeled_data['label']\n",
    "    label_counter = collections.Counter(all_labels)\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in label_counter.items():\n",
    "        print(f\"Label {label}: {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "        \n",
    "    # Check for label imbalance\n",
    "    min_label_count = min(label_counter.values())\n",
    "    max_label_count = max(label_counter.values())\n",
    "    imbalance_ratio = max_label_count / min_label_count if min_label_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"WARNING: Severe label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights or resampling.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(f\"WARNING: Moderate label imbalance detected (ratio: {imbalance_ratio:.2f}). Consider using class weights.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splitting",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Now let's split our data into training and validation sets. We'll use scikit-learn's train_test_split to avoid the ClassLabel issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert to pandas DataFrame for easier manipulation\n",
    "    df = labeled_data.to_pandas()\n",
    "    \n",
    "    # Split using scikit-learn's train_test_split with stratification\n",
    "    train_df, val_df = sklearn_train_test_split(\n",
    "        df, \n",
    "        test_size=0.1, \n",
    "        random_state=42, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Convert back to HuggingFace datasets\n",
    "    train_data = Dataset.from_pandas(train_df)\n",
    "    val_data = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Verify label distribution after split\n",
    "    train_label_counter = collections.Counter(train_data['label'])\n",
    "    val_label_counter = collections.Counter(val_data['label'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training label distribution: {dict(train_label_counter)}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation label distribution: {dict(val_label_counter)}\")\n",
    "    \n",
    "    # Check if dataset is large (might cause memory issues)\n",
    "    if len(train_data) > 10000:\n",
    "        print(\"\\nWARNING: You are training on a large dataset.\")\n",
    "        print(\"This may require significant memory, especially when using a GPU.\")\n",
    "        print(\"Consider reducing batch size or using gradient accumulation if you encounter memory issues.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9",
   "metadata": {},
   "source": [
    "## Optimized Tokenization\n",
    "\n",
    "Now we need to tokenize our code samples. We'll use the CodeBERT tokenizer to convert the Swift code into token IDs that the model can understand. This implementation is optimized for speed and memory efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e349-613c-4353-9896-856f15daf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the Swift code samples with optimized settings.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized examples\n",
    "    \"\"\"\n",
    "    # Tokenize the code content with optimized settings\n",
    "    # - No return_tensors=\"pt\" for memory efficiency\n",
    "    # - padding=False for dynamic padding later with DataCollator\n",
    "    # - truncation=True to handle long sequences\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        padding=False,  # We'll use dynamic padding with DataCollator\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process the data with progress bars and optimized settings\n",
    "    tokenized_train_data = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in larger batches for speed\n",
    "        remove_columns=[col for col in train_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing training data\",  # This adds a progress bar\n",
    "        num_proc=NUM_WORKERS if NUM_WORKERS > 0 else None  # Use multiprocessing if available\n",
    "    )\n",
    "    \n",
    "    tokenized_val_data = val_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in larger batches for speed\n",
    "        remove_columns=[col for col in val_data.column_names if col != 'label'],\n",
    "        desc=\"Tokenizing validation data\",  # This adds a progress bar\n",
    "        num_proc=NUM_WORKERS if NUM_WORKERS > 0 else None  # Use multiprocessing if available\n",
    "    )\n",
    "    \n",
    "    # Set format for pytorch\n",
    "    tokenized_train_data = tokenized_train_data.with_format(\"torch\")\n",
    "    tokenized_val_data = tokenized_val_data.with_format(\"torch\")\n",
    "    \n",
    "    print(\"Training data after tokenization:\")\n",
    "    print(tokenized_train_data)\n",
    "    print(\"\\nValidation data after tokenization:\")\n",
    "    print(tokenized_val_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-preparation",
   "metadata": {},
   "source": [
    "## Model Preparation with LoRA\n",
    "\n",
    "Now that our data is ready, let's load the CodeBERT model and configure it for sequence classification with LoRA for efficient fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the CodeBERT model for sequence classification (2 classes)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        num_labels=2,\n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        gradient_checkpointing=True if use_gpu or use_tpu else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Base model type: {base_model.__class__.__name__}\")\n",
    "    \n",
    "    # Calculate class weights for imbalanced dataset\n",
    "    label_counts = collections.Counter(train_data['label'])\n",
    "    total_samples = len(train_data)\n",
    "    class_weights = {label: total_samples / (len(label_counts) * count) for label, count in label_counts.items()}\n",
    "    print(f\"Class weights for handling imbalance: {class_weights}\")\n",
    "    \n",
    "    # Convert class weights to tensor for loss function\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [class_weights[i] for i in range(len(class_weights))],\n",
    "        dtype=torch.float\n",
    "    ).to(device)\n",
    "    \n",
    "    # Configure LoRA for efficient fine-tuning\n",
    "    # For RoBERTa/CodeBERT, we target the attention layers (query and value)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "        r=LORA_R,                    # Rank of the update matrices\n",
    "        lora_alpha=LORA_ALPHA,       # Scaling factor\n",
    "        lora_dropout=LORA_DROPOUT,   # Dropout probability\n",
    "        # Target the attention layers in CodeBERT (which is RoBERTa-based)\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        bias=\"none\",                 # Don't train bias parameters\n",
    "        modules_to_save=[\"classifier\"],  # Save the classifier head\n",
    "    )\n",
    "    \n",
    "    # Create the LoRA model\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters to verify LoRA setup\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    if not use_tpu:  # For TPU, the Trainer will handle device placement\n",
    "        model.to(device)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now let's define our training arguments and evaluation metrics with optimized settings for LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics during evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    try:\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Calculate multiple metrics\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Calculate per-class metrics for better understanding\n",
    "        per_class_precision, per_class_recall, per_class_f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average=None\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for i, (p, r, f) in enumerate(zip(per_class_precision, per_class_recall, per_class_f1)):\n",
    "            result[f'precision_class_{i}'] = p\n",
    "            result[f'recall_class_{i}'] = r\n",
    "            result[f'f1_class_{i}'] = f\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments with optimized settings for LoRA\n",
    "try:\n",
    "    # With LoRA, we can use a higher learning rate\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/codebert-swift-lora\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,  # Keep only the 3 best checkpoints\n",
    "        learning_rate=1e-4,  # Higher learning rate for LoRA\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "        # TPU-specific configurations\n",
    "        tpu_num_cores=8 if use_tpu else None,  # 8 cores for TPU v2/v3\n",
    "        dataloader_drop_last=True if use_tpu else False,  # Important for TPU\n",
    "        # Memory and performance optimizations\n",
    "        fp16=use_mixed_precision,  # Use mixed precision when available\n",
    "        dataloader_num_workers=NUM_WORKERS,\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        max_grad_norm=1.0,\n",
    "        # Warmup steps for learning rate scheduler\n",
    "        warmup_ratio=0.1,  # Warm up over 10% of training steps\n",
    "        # Reporting\n",
    "        report_to=[\"tensorboard\"],\n",
    "        # Optimizer settings\n",
    "        optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n",
    "        # Avoid OOM errors by not storing gradients for all steps\n",
    "        gradient_checkpointing=True if (use_gpu or use_tpu) else False,\n",
    "        # Avoid unnecessary memory usage\n",
    "        remove_unused_columns=True,\n",
    "        # Disable tqdm progress bars in favor of our own reporting\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Training arguments configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring training arguments: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer with data collator and callbacks\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_val_data,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,  # Added data collator for dynamic padding\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Added early stopping\n",
    "    )\n",
    "    \n",
    "    print(\"Trainer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating trainer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-recovery",
   "metadata": {},
   "source": [
    "## Checkpoint Recovery\n",
    "\n",
    "Let's add improved checkpoint recovery logic to resume training if it was interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-for-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints using HuggingFace's built-in function\n",
    "try:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use HuggingFace's get_last_checkpoint function\n",
    "    latest_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found existing checkpoint at {latest_checkpoint}. Training will resume from this point.\")\n",
    "    else:\n",
    "        print(\"No existing checkpoint found. Training will start from scratch.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for checkpoints: {e}\")\n",
    "    latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## Training the Model with LoRA\n",
    "\n",
    "Now let's train our CodeBERT model for Swift code classification with LoRA for efficient fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with checkpoint recovery and memory optimization\n",
    "try:\n",
    "    # Clear memory before training\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Starting model training with LoRA...\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    print(f\"Training completed. Metrics: {train_result.metrics}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model()\n",
    "    print(\"Final model saved.\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "try:\n",
    "    print(\"Evaluating model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    trainer.log_metrics(\"eval\", eval_results)\n",
    "    trainer.save_metrics(\"eval\", eval_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-export",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Now let's export our model for deployment. We'll merge the LoRA weights with the base model for easier deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to a specific directory\n",
    "try:\n",
    "    export_dir = \"./exported-model-lora\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Merge LoRA weights with the base model\n",
    "    print(\"Merging LoRA weights with base model...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save the merged model\n",
    "    merged_model.save_pretrained(export_dir)\n",
    "    tokenizer.save_pretrained(export_dir)\n",
    "    \n",
    "    # Save model configuration and metadata\n",
    "    model_info = {\n",
    "        \"model_name\": \"CodeBERT-Swift-LoRA\",\n",
    "        \"base_model\": MODEL_ID,\n",
    "        \"task\": \"binary_classification\",\n",
    "        \"labels\": [\"Not Package.swift\", \"Package.swift\"],\n",
    "        \"metrics\": eval_results,\n",
    "        \"lora_config\": {\n",
    "            \"rank\": LORA_R,\n",
    "            \"alpha\": LORA_ALPHA,\n",
    "            \"dropout\": LORA_DROPOUT,\n",
    "            \"target_modules\": [\"query\", \"value\"]\n",
    "        },\n",
    "        \"training_params\": {\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"learning_rate\": training_args.learning_rate,\n",
    "            \"epochs\": training_args.num_train_epochs,\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "            \"training_samples\": len(tokenized_train_data),\n",
    "            \"validation_samples\": len(tokenized_val_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(export_dir, \"model_info.json\"), \"w\") as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "        \n",
    "    print(f\"Model exported to {export_dir}\")\n",
    "    \n",
    "    # Also save the LoRA adapter separately for future use\n",
    "    lora_dir = \"./lora-adapter\"\n",
    "    os.makedirs(lora_dir, exist_ok=True)\n",
    "    model.save_pretrained(lora_dir)\n",
    "    print(f\"LoRA adapter saved to {lora_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-upload",
   "metadata": {},
   "source": [
    "## Upload to Dropbox (Optional)\n",
    "\n",
    "If you want to upload the model to Dropbox for easy access, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-dropbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload a file to Dropbox\n",
    "def upload_to_dropbox(file_path, dropbox_path, access_token):\n",
    "    \"\"\"Upload a file to Dropbox.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to upload\n",
    "        dropbox_path: Path in Dropbox where the file should be uploaded\n",
    "        access_token: Dropbox access token\n",
    "        \n",
    "    Returns:\n",
    "        Response from Dropbox API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import dropbox\n",
    "        dbx = dropbox.Dropbox(access_token)\n",
    "        \n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
    "            \n",
    "            if file_size <= chunk_size:\n",
    "                # Small file, upload in one go\n",
    "                return dbx.files_upload(f.read(), dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "            else:\n",
    "                # Large file, use chunked upload\n",
    "                upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
    "                cursor = dropbox.files.UploadSessionCursor(\n",
    "                    session_id=upload_session_start_result.session_id,\n",
    "                    offset=f.tell()\n",
    "                )\n",
    "                commit = dropbox.files.CommitInfo(path=dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
    "                \n",
    "                while f.tell() < file_size:\n",
    "                    if (file_size - f.tell()) <= chunk_size:\n",
    "                        # Last chunk\n",
    "                        return dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)\n",
    "                    else:\n",
    "                        # Intermediate chunk\n",
    "                        dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)\n",
    "                        cursor.offset = f.tell()\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Dropbox: {e}\")\n",
    "        raise\n",
    "\n",
    "# To use this function, uncomment and provide your Dropbox access token\n",
    "# DROPBOX_ACCESS_TOKEN = \"your_access_token_here\"\n",
    "# upload_to_dropbox(\"./exported-model-lora.zip\", \"/CodeBERT-Swift-LoRA/model.zip\", DROPBOX_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-example",
   "metadata": {},
   "source": [
    "## Inference Example\n",
    "\n",
    "Let's create a simple inference example to demonstrate how to use the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(code_text, model, tokenizer, device):\n",
    "    \"\"\"Make a prediction with the trained model.\n",
    "    \n",
    "    Args:\n",
    "        code_text: Swift code text to classify\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Prediction label and confidence score\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        code_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get prediction and confidence\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0, predicted_class].item()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example Swift code\n",
    "example_code = \"\"\"\n",
    "import Foundation\n",
    "\n",
    "struct Person {\n",
    "    let name: String\n",
    "    let age: Int\n",
    "    \n",
    "    func greet() -> String {\n",
    "        return \"Hello, my name is \\(name) and I am \\(age) years old.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "let john = Person(name: \"John\", age: 30)\n",
    "print(john.greet())\n",
    "\"\"\"\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    predicted_class, confidence = predict_with_model(example_code, merged_model, tokenizer, device)\n",
    "    print(f\"Prediction: {'Package.swift' if predicted_class == 1 else 'Not Package.swift'}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've fine-tuned the CodeBERT model on Swift code using LoRA for efficient training. The model can now be used for code understanding tasks related to Swift.\n",
    "\n",
    "Key optimizations implemented:\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)** for efficient fine-tuning with significantly fewer parameters\n",
    "2. **Mixed precision training** for faster computation\n",
    "3. **Gradient checkpointing** to reduce memory usage\n",
    "4. **Dynamic batch sizing** based on available hardware\n",
    "5. **Efficient data loading** with multiprocessing\n",
    "6. **Memory management** with garbage collection and cache clearing\n",
    "7. **Optimized tokenization** with batched processing\n",
    "8. **Proper stratification** using scikit-learn instead of datasets library\n",
    "9. **Improved checkpoint handling** for reliable training resumption\n",
    "\n",
    "The model is now ready for deployment in your applications!\n",
    "\n",
    "### Benefits of LoRA\n",
    "\n",
    "By using LoRA, we achieved:\n",
    "- **Faster training**: Training completes in significantly less time\n",
    "- **Lower memory usage**: Reduced memory footprint allows for larger batch sizes\n",
    "- **Smaller model size**: The adapter is much smaller than a fully fine-tuned model\n",
    "- **Comparable performance**: Results are similar to full fine-tuning\n",
    "\n",
    "This approach is ideal for efficiently adapting large pre-trained models to specific tasks with limited computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}